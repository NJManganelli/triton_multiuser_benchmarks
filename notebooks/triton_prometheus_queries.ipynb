{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2c7aa-75a0-45aa-9cc2-e7214dbb41ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import json\n",
    "import prometheus_api_client\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from prometheus_api_client.metric_range_df import MetricRangeDataFrame\n",
    "from prometheus_api_client.metric_snapshot_df import MetricSnapshotDataFrame\n",
    "from prometheus_api_client.metrics_list import MetricsList\n",
    "from prometheus_api_client.utils import parse_datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c5c44-f3e4-437d-a7cb-bfe5be8b634f",
   "metadata": {},
   "source": [
    "## We connect to the Prometheus Metrics server which logs the data for the EAF's Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8010b-3b64-4c9c-9733-933144858a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prom = PrometheusConnect(url=\"http://lsdataitb.fnal.gov:9009/prometheus\", disable_ssl=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68795db7-31d5-4b5e-8e78-d14ab9371b80",
   "metadata": {},
   "source": [
    "This lists how many metrics are available, and in particular the ones for the GPU MIG instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c85d9-0dc5-42f0-b43d-0476e5836864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xxx = prom.all_metrics()\n",
    "len(xxx)\n",
    "[xx for xx in xxx if \"GPU\" in xx or \"DCGM\" in xx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b94a07-2fe2-48fb-8ce2-7fc70f69f093",
   "metadata": {},
   "source": [
    "## Main Query Function\n",
    "This uses the PromQL language to get metrics recorded between the first and last timestamp. To prevent overloading the prometheus server,\n",
    "the timestamps should be broken up into pairs that are on the order of hours to a day or two (so to get a week's worth of logs, do 7 1-day increments as a ```list[(t0, t1), (t1, t2), (t2, t3), (t3, t4), (t4, t5), (t5, t6)]```\n",
    "These will be concatenated so that the returned results dictionary contains, for each query, a single pandas datafram for (t0, t6)\n",
    "\n",
    "An importan caveat about prometheus metrics are the disparate collection and timing aspects. Each metric tends to log one figure of merit, with many labels for association to a particular task or resource type. As an example, the number of inferences computed to this point may be logged (with associated timestamp), but the nearly-coincident inference request duration may happen slightly sooner, later, or not at all. This point is important, as missing metrics are not rare. Many of these are logged at the EAF with a frequency of 15s or 30s. The prometheus developers recommend aggregating metris in a time-window 4x larger than the collection frequency. As such, 60s or 120s should be chosen as the ```step``` value in the function to avoid noisy and missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f2046-6e41-4340-b8cc-94ebfd43a5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function for getting queries of many GPU and Triton server metrics. Inputs are a list of timestamp tuples,\n",
    "# which can be parsed by the prometheus_api_client.utils.parse_datetime function. This can understand timestamps formatted like\n",
    "# \"2023-03-30 at 16:00:00 MDT\"\n",
    "# The step is the 'time-window' over which each query will be divided. This should be ~4x as long as the longest frequency for metric-gather\n",
    "def get_all_queries(timestamp_tuples, step):\n",
    "    # A dictionary for our results\n",
    "    results = {}\n",
    "    # Tuples of the queries we'll make, for debugging and info\n",
    "    queries = []\n",
    "    \n",
    "    # Some queries are best created after understanding which unique models+version have been run in the triton servers\n",
    "    # and which GPU instances have been active. These are then used to formulate model/version-specific and GPU-specific stats\n",
    "    unique_model_versions = None\n",
    "    unique_gpu_instances = None\n",
    "    \n",
    "    #Basic queries. Some of them are used as proxies to figure out the unqique queries to make later, like the \"gpu_tensor_util\" below\n",
    "    for key, query in {\n",
    "        \"num_instances\": \"count((sum by(pod) (delta(nv_inference_request_success[\"+step+\"]))) > 0)\",\n",
    "        \"inf_rate_net\":\"sum (rate(nv_inference_count[\"+step+\"]))\",\n",
    "        \"inf_rate_bypod\":\"sum by(pod) (rate(nv_inference_count[\"+step+\"]))\",\n",
    "        \"inf_rate\":\"sum by(model, version, pod) (rate(nv_inference_count[\"+step+\"]))\",\n",
    "        \"inf_cache_hit_rate\":\"sum by(model, version, pod) (rate(nv_cache_num_hits_per_model[\"+step+\"]))\",\n",
    "        \"inf_reqs_net\":\"sum(rate(nv_inference_request_success[\"+step+\"]))\",\n",
    "        \"inf_reqs_bypod\":\"sum by(pod) (rate(nv_inference_request_success[\"+step+\"]))\",\n",
    "        \"inf_reqs\":\"sum by(model, version, pod) (rate(nv_inference_request_success[\"+step+\"]))\",\n",
    "        \"inf_req_dur_net\": \"avg (delta(nv_inference_request_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_que_dur_net\": \"avg (delta(nv_inference_queue_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_inp_dur_net\": \"avg (delta(nv_inference_compute_input_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_inf_dur_net\": \"avg (delta(nv_inference_compute_infer_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_out_dur_net\": \"avg (delta(nv_inference_compute_output_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_req_dur\": \"avg by(model, version, pod) (delta(nv_inference_request_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_que_dur\": \"avg by(model, version, pod) (delta(nv_inference_queue_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_inp_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_input_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_inf_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_infer_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_out_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_output_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"gpu_tensor_util\": \"sum by(device,GPU_I_ID,instance) (avg_over_time (DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        \"gpu_dram_util\": \"sum by(device,GPU_I_ID,instance) (avg_over_time (DCGM_FI_PROF_DRAM_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        #\"inf_cache_hits\": \"avg by(model, version, pod) (delta(nv_cache_num_hits_per_model[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        }.items():\n",
    "        # Build an empty list for these results; after iterating through all the timestamp pairs, they'll be concatenated together\n",
    "        results[key] = []\n",
    "        # Log the queries, as they're easier to parse after being resolved fully\n",
    "        queries.append((key, query))\n",
    "        # This function executes a query for each timestamp pair, for each key:query\n",
    "        for st, et in timestamp_tuples:\n",
    "            test_inp = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=parse_datetime(st),\n",
    "                end_time=parse_datetime(et),\n",
    "                step=step\n",
    "            )\n",
    "            # Queries are converted to a pandas dataframe\n",
    "            df = MetricRangeDataFrame(test_inp)\n",
    "            results[key].append(df)\n",
    "        # Dataframes are concatenated together along the time (index value) axis\n",
    "        results[key] = pd.concat(results[key], axis=0)\n",
    "        \n",
    "        # If we've performed a query that stores model/version info and GPU instance info, respectively, we can \n",
    "        # Create a set of unique ones for the next two sets of queries\n",
    "        if unique_model_versions is None and hasattr(results[key], \"model\") and hasattr(results[key], \"version\"):\n",
    "            unique_model_versions = set((results[key].model+\"/\"+results[key].version).values)\n",
    "        # At the EAF, the device ('nvidiaX' where X is 0...4 for example), GPU instance ID (enumeration)\n",
    "        # and the instance (IP address of host machine) are sufficient to make a unique identifier\n",
    "        if unique_gpu_instances is None and hasattr(results[key], \"GPU_I_ID\"):\n",
    "            unique_gpu_instances = set((results[key].device+\"/\"+results[key].GPU_I_ID+\"/\"+results[key].instance).values)\n",
    "    # Here we build the model-specific queries, getting both the number of unique number of Triton instances that served \n",
    "    # inference requests for this model, ad well as the inference rate of that model across all Triton instances active per time step\n",
    "    model_queries = {\"num_instances_\"+model_version: \"count((sum by(pod) (delta(nv_inference_request_success{model='\"+\n",
    "                     model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'}[\"+step+\"]))) > 0)\"\n",
    "                     for model_version in unique_model_versions}\n",
    "    model_queries.update(\n",
    "        {\"inf_rate_\"+model_version: \"sum (rate(nv_inference_count{model='\"+\n",
    "         model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'}[\"+step+\"]))\"\n",
    "         for model_version in unique_model_versions})\n",
    "    for key, query in model_queries.items():\n",
    "        queries.append((key, query))\n",
    "        results[key] = []\n",
    "        for st, et in timestamp_tuples:\n",
    "            test_inp = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=parse_datetime(st),\n",
    "                end_time=parse_datetime(et),\n",
    "                step=step\n",
    "            )\n",
    "            # The query could be empty, as a model only served in a portion of the total timerange could be inactive in some timestamp-pairs.\n",
    "            # We will deal with broadcasting these dataframes with missing values later\n",
    "            if len(test_inp) > 0:\n",
    "                df = MetricRangeDataFrame(test_inp)\n",
    "                results[key].append(df)\n",
    "        if len(results[key]) > 0:\n",
    "            results[key] = pd.concat(results[key], axis=0)\n",
    "        else:\n",
    "            # If somehow we got no results for this model query, remove it from the dictionary and avoid iterating over it later\n",
    "            results.pop(key)\n",
    "            unique_model_versions.remove(key.split(\"_instances_\")[1])\n",
    "            \n",
    "    # Now we gather the GPU metrics. The two most interesting ones for us are the DCGM_FI_PROF_PIPE_TENSOR_ACTIVE and \n",
    "    # DCGM_FI_PROF_DRAM_ACTIVE. The former measures how much of the compute resources (the Tensor Cores) are active, on average, in a time period\n",
    "    # If the utilization is 50%, this could mean that the tensor cores for this GPU (slice) are 100% active for 50% of the time, 50% active for\n",
    "    # 100% of the time, or any combination of activity_percent * time_active_percent that gives that product.\n",
    "    gpu_queries = {\"gpu_tensor_util_\"+str(mg): \"sum (avg_over_time(DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{\"+\n",
    "                   \"exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\"+\n",
    "                   \"device='\"+gpu_inst.split(\"/\")[0]+\"',GPU_I_ID='\"+gpu_inst.split(\"/\")[1]+\"',instance='\"+gpu_inst.split(\"/\")[2]+\"'}[\"+step+\"]))\" for mg, gpu_inst in enumerate(unique_gpu_instances)}\n",
    "    # An example of how additional labels can filter out non-matching queries, if we do \n",
    "    # DCGM_FI_PROF_DRAM_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\n",
    "    #                          device='nvidia2',GPU_I_ID='3',instance='110.4.29.45'}[120s]\n",
    "    # We'll only get metrics from that specific device, if it has a running instance with that IP, and a running GPU instance matching it\n",
    "    # In this case, for each timestep, it'll get a 'vector' of instantaenous measurements within 120s\n",
    "    # The avg_over_time function then measures the average over time of that 'vector' and produces a scalar result\n",
    "    # The scalar result may not be unique for a given timestamp, there can be other labels attached, and a final avg is taken over all\n",
    "    # of those\n",
    "    gpu_queries.update(\n",
    "        {\"gpu_dram_util_\"+str(mg): \"avg (avg_over_time(DCGM_FI_PROF_DRAM_ACTIVE{\"+\n",
    "         \"exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\"+\n",
    "        \"device='\"+gpu_inst.split(\"/\")[0]+\"',GPU_I_ID='\"+gpu_inst.split(\"/\")[1]+\"',instance='\"+gpu_inst.split(\"/\")[2]+\"'}[\"+step+\"]))\"\n",
    "         for mg, gpu_inst in enumerate(unique_gpu_instances)})\n",
    "    for key, query in gpu_queries.items():\n",
    "        queries.append((key, query))\n",
    "        results[key] = []\n",
    "        for st, et in timestamp_tuples:\n",
    "            test_inp = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=parse_datetime(st),\n",
    "                end_time=parse_datetime(et),\n",
    "                step=step\n",
    "            )\n",
    "            if len(test_inp) > 0:\n",
    "                df = MetricRangeDataFrame(test_inp)\n",
    "                results[key].append(df)\n",
    "        if len(results[key]) > 0:\n",
    "            results[key] = pd.concat(results[key], axis=0)\n",
    "            #print(key)\n",
    "        else:\n",
    "            #print(f\"results empty for {key}\")\n",
    "            results.pop(key)\n",
    "            unique_gpu_instances.remove(key.split(\"_util_\")[1])\n",
    "    return results, queries, unique_model_versions, unique_gpu_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369b7e2-d0a7-426a-9023-7f5dbb33d0e8",
   "metadata": {},
   "source": [
    "In addition to the query dataframes, a list of the key:query pairs and the different model/versions active, and GPU MIG slices active, will be recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428998de-695a-4e4b-97d7-59ec468afbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results, queries, unique_model_versions, unique_gpu_instances = get_all_queries([(\"2023-03-30 at 16:00:00 MDT\", \"2023-03-30 at 19:00:00 MDT\"),], step=\"60s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb863da4-dc44-4e44-863a-962e306b3f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_model_versions, unique_gpu_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4875702-3e44-4a58-90d9-517ef2d5e773",
   "metadata": {},
   "source": [
    "An example result, the average percent utilization of the 1st GPU tensor pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7245fa8-2eed-4b0a-8ff4-972c9579c6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[\"gpu_tensor_util_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bbd25-2e29-45d9-b043-56517d5d0c4d",
   "metadata": {},
   "source": [
    "## Converting to a unified dataframe\n",
    "This function takes the subset of results that can be concatenated into new columns,\n",
    "including the inference rate (with breakdownds by model), the timing of the inference request (overall request time, as well as broken down into queue time, input time, compute time, and output time), and the GPU dram and tensor utilization. See NVidia docs for more information on how these quantities are calculated and stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12b5d5-22f2-4f06-99ae-c9262b11013d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_results_to_df(results):\n",
    "    # This iteratively walks through some of the dataframes that are compatible and aggregates results into a \n",
    "    # unified dataframe. In each dataframe, the join call, in combination with how='left', means that results are broadcast\n",
    "    # and filled with NaN wherever results may be missing from the second of the two dataframes.\n",
    "    # For this reason, the 'inf_rate_net' which should have a valid value for all timestamps is used as the base.\n",
    "    i0 = results[\"inf_rate_net\"].join(results[\"num_instances\"],\n",
    "                                      how=\"left\", \n",
    "                                      rsuffix=\"_num_instances\",\n",
    "                                     )\n",
    "    # We use the rsuffix and lsuffix to convert column names from 'value' to one that is understandable/parseable later on. \n",
    "    i0 = i0.join(results[\"inf_reqs_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_reqs_net\")\n",
    "    i0 = i0.join(results[\"inf_req_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_req_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_que_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_que_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_inp_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_inp_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_inf_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_inf_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_out_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_out_dur_net\")\n",
    "    \n",
    "    #Add the model metrics, using some suffix parsing to make it into num_instances_X or rate_X where X is the model name\n",
    "    for model in unique_model_versions:\n",
    "        itemp = results[\"inf_rate_\" + model].join(results[\"num_instances_\" + model],\n",
    "                                                  how=\"left\",\n",
    "                                                  rsuffix=\"_num_instances_\"+model.split(\"/\")[0],\n",
    "                                                  lsuffix=\"_rate_\"+model.split(\"/\")[0],\n",
    "                                                 )\n",
    "        i0 = i0.join(itemp, how=\"left\")\n",
    "        \n",
    "    #Add the GPU Instance metrics, including GPU instance enumeration\n",
    "    for mg, gpu in enumerate(unique_gpu_instances):\n",
    "        results[\"gpu_tensor_util_\" + str(mg)].fillna(0, inplace=True)\n",
    "        results[\"gpu_dram_util_\" + str(mg)].fillna(0, inplace=True)\n",
    "        itemp = results[\"gpu_tensor_util_\" + str(mg)].join(results[\"gpu_dram_util_\" + str(mg)],\n",
    "                                                  how=\"left\",\n",
    "                                                  rsuffix=\"_gpu_dram_util_\"+str(mg),\n",
    "                                                  lsuffix=\"_gpu_tensor_util_\"+str(mg),\n",
    "                                                 )\n",
    "        i0 = i0.join(itemp, how=\"left\")\n",
    "\n",
    "    #Get rid of the \"value\" in column names, and fill NaN values with 0 everywhere\n",
    "    i0.rename(columns={\"value\": \"rate\"}, inplace=True)\n",
    "    i0.rename(columns={col:col[6:] for col in i0.columns if col.startswith(\"value_\")}, inplace=True)\n",
    "    i0.fillna(0, inplace=True)\n",
    "    \n",
    "    # Aggregate some stats for models\n",
    "    # The summed rate and total inference rate should match, otherwise we've double-counted something\n",
    "    # The summed instances may NOT match: if a model is active on 5 of 10 servers in a timestep, and another is active on 7 of 10\n",
    "    # Then there will be '12' active instances in that timestep, net. This number divided by the net_instances\n",
    "    # Therefore gives a measure of the 'average' model concurrency in a timestep. 10 net_instances and 70 summed_intstances\n",
    "    # would indicate each instances was serving 7 models at some point in that timestep (but this is a lossy gathering of information,\n",
    "    # 6 models could do one inference request while the last model is responsible for all of the remainder of thousands of requests.\n",
    "    valid_model_keys = [col for col in i0.columns if col.startswith(\"rate_\") and col.replace(\"rate_\", \"num_instances_\") in i0.columns]\n",
    "    i0[\"summed_rate\"] = sum([i0[col] for col in valid_model_keys])\n",
    "    i0[\"summed_instances\"] = sum([i0[col.replace(\"rate_\", \"num_instances_\")] for col in valid_model_keys])\n",
    "    \n",
    "    # Aggregate some stats for GPU instances\n",
    "    valid_gpu_keys = [col for col in i0.columns if col.startswith(\"gpu_tensor_util\") and col.replace(\"tensor\", \"dram\") in i0.columns]\n",
    "    i0[\"summed_gpu_tensor_util\"] = sum([i0[col] for col in valid_gpu_keys])\n",
    "    i0[\"summed_gpu_dram_util\"] = sum([i0[col.replace(\"tensor\", \"dram\")] for col in valid_model_keys])\n",
    "    return i0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a8b3d-7d38-4b98-8a3a-2d283f4990e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i0 = convert_results_to_df(results)\n",
    "i0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e02198-b119-4df0-a73d-66e8770f5860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save results in a pickle file for later\n",
    "import pickle\n",
    "with open(f\"triton_metrics_test.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(i0, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8f557-ff61-47c7-b643-4740e11d6779",
   "metadata": {},
   "source": [
    "## A few simple plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243ca52-500b-408c-91c3-ab5393c1b1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(i0.index.values, i0.rate.values)\n",
    "scale_value = max(i0.rate.values)/max(i0.summed_gpu_tensor_util)\n",
    "plt.plot(i0.index.values, i0.summed_gpu_tensor_util.values*scale_value, color=\"tab:red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6c219-49ca-49d6-ab2a-e9c00707dcd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the rate versus number of instances, where at least 1 active instance is serving results\n",
    "plt.scatter(\"num_instances\", \"rate\", data=i0[i0.num_instances > 0], color=\"tab:red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58424494-1247-4eee-9b46-7867e6ef21b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the same thing, but specific to the pn_demo model\n",
    "plt.scatter(\"num_instances_pn_demo\", \"rate_pn_demo\", data=i0[i0.num_instances > 0], color=\"tab:blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ddc95-759c-4451-ae45-ab313e4b42c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(\"num_instances_svj_tch_gnn\", \"rate_svj_tch_gnn\", data=i0[i0.num_instances > 0], color=\"tab:green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39053bf5-269d-4994-bf75-47aabc2b055b",
   "metadata": {},
   "source": [
    "## Concurrency\n",
    "How can we measure how many models are active per Triton server? The ```num_instances``` is how many actives servers there are.\n",
    "The variables ```summed_instances``` is the sum of each model's active ```num_instances```. If the values are equal, then concurrency is low\n",
    "(when defined as the number of ML models being run on an individual server). If ```summed_instances >> num_instances```, that indicates that each triton server is tending to actively serve requests from multiple models in a given timespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b912b07-fbd3-431b-8d1c-330b63f73aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Concurrency question: if models tend to gravitate to their own instances, summed instances ~ num_instances\n",
    "#If concurrency is as high as possible, summed instances ~ avg_num_models * num_instances\n",
    "ii = i0[i0.num_instances > 0].summed_instances/i0[i0.num_instances > 0].num_instances\n",
    "print(np.mean(ii), np.max(ii), np.min(ii))\n",
    "print(np.sqrt(np.var(ii)))\n",
    "\n",
    "#Consistency check: summed rate should always add to net rate!\n",
    "kk = i0[i0.num_instances > 0].summed_rate/i0[i0.num_instances > 0].rate\n",
    "print(np.mean(kk), np.max(kk), np.min(kk))\n",
    "print(np.sqrt(np.var(kk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e125dc-31af-4d94-828b-a588d07877de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea for triton",
   "language": "python",
   "name": "coffea-triton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
