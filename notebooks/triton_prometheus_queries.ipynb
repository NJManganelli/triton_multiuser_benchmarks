{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2c7aa-75a0-45aa-9cc2-e7214dbb41ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import time\n",
    "import json\n",
    "import prometheus_api_client\n",
    "import hist\n",
    "import copy\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from prometheus_api_client.metric_range_df import MetricRangeDataFrame\n",
    "from prometheus_api_client.metric_snapshot_df import MetricSnapshotDataFrame\n",
    "from prometheus_api_client.metrics_list import MetricsList\n",
    "from prometheus_api_client.utils import parse_datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich.progress import track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c5c44-f3e4-437d-a7cb-bfe5be8b634f",
   "metadata": {},
   "source": [
    "## We connect to the Prometheus Metrics server which logs the data for the EAF's Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8010b-3b64-4c9c-9733-933144858a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prom = PrometheusConnect(url=\"http://lsdataitb.fnal.gov:9009/prometheus\", disable_ssl=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68795db7-31d5-4b5e-8e78-d14ab9371b80",
   "metadata": {},
   "source": [
    "This lists how many metrics are available, and in particular the ones for the GPU MIG instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c85d9-0dc5-42f0-b43d-0476e5836864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xxx = prom.all_metrics()\n",
    "len(xxx)\n",
    "[xx for xx in xxx if \"GPU\" in xx or \"DCGM\" in xx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b94a07-2fe2-48fb-8ce2-7fc70f69f093",
   "metadata": {},
   "source": [
    "## Main Query Function\n",
    "This uses the PromQL language to get metrics recorded between the first and last timestamp. To prevent overloading the prometheus server,\n",
    "the timestamps should be broken up into pairs that are on the order of hours to a day or two (so to get a week's worth of logs, do 7 1-day increments as a ```list[(t0, t1), (t1, t2), (t2, t3), (t3, t4), (t4, t5), (t5, t6)]```\n",
    "These will be concatenated so that the returned results dictionary contains, for each query, a single pandas datafram for (t0, t6)\n",
    "\n",
    "An importan caveat about prometheus metrics are the disparate collection and timing aspects. Each metric tends to log one figure of merit, with many labels for association to a particular task or resource type. As an example, the number of inferences computed to this point may be logged (with associated timestamp), but the nearly-coincident inference request duration may happen slightly sooner, later, or not at all. This point is important, as missing metrics are not rare. Many of these are logged at the EAF with a frequency of 15s or 30s. The prometheus developers recommend aggregating metris in a time-window 4x larger than the collection frequency. As such, 60s or 120s should be chosen as the ```step``` value in the function to avoid noisy and missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc99af-0a1b-4744-a0c0-983160bb90fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = None\n",
    "import pickle\n",
    "first = None\n",
    "last = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79dc90-b91d-42e4-aae5-d6a09fb359e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"UnitaryClient_88parcels_trial1_benchmark00.pickle\", \"rb\") as pf:\n",
    "    x = pickle.load(pf)\n",
    "    all_logs = []\n",
    "    all_z = []\n",
    "    for xx in x:\n",
    "        all_logs += xx['worklogs']\n",
    "        z = [xxx.hostname + str(xxx.pid) for xxx in xx['worklogs']]\n",
    "        dts_s = min([xxx.start_time for xxx in xx['worklogs']])\n",
    "        dts_e = max([xxx.end_time for xxx in xx['worklogs']])\n",
    "        #first = dts_s\n",
    "        #last = dts_e\n",
    "        #print(dts_s, dts_e)\n",
    "        #print(parse_datetime(dts_s), parse_datetime(dts_e))\n",
    "        #print(set(z))\n",
    "        #all_z += z\n",
    "    #print(len(set(all_z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b87f44-6d2b-4714-b235-6d862ef122c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parse_datetime(\"2023-04-03 at 21:12:07 MDT\") == parse_datetime(\"2023-04-03 at 22:12:07 CDT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2339ad0-6832-4f72-b8af-f30bc1dc4cab",
   "metadata": {},
   "source": [
    "# datetimeff = get_all_queries([(datetime.fromtimestamp(time.mktime(first)), datetime.fromtimestamp(time.mktime(last)))], \"120s\")\n",
    "'dst',\n",
    " 'fold',\n",
    " 'fromisocalendar',\n",
    " 'fromisoformat',\n",
    " 'fromordinal',\n",
    " 'fromtimestamp',\n",
    " 'hour',\n",
    " 'isocalendar',\n",
    " 'isoformat',\n",
    " 'isoweekday',\n",
    " 'max',\n",
    " 'microsecond',\n",
    " 'min',\n",
    " 'minute',\n",
    " 'month',\n",
    " 'now',\n",
    " 'replace',\n",
    " 'resolution',\n",
    " 'second',\n",
    " 'strftime',\n",
    " 'strptime',\n",
    " 'time',\n",
    " 'timestamp',\n",
    " 'timetuple',\n",
    " 'timetz',\n",
    " 'today',\n",
    " 'toordinal',\n",
    " 'tzinfo',\n",
    " 'tzname',\n",
    " 'utcfromtimestamp',\n",
    " 'utcnow',\n",
    " 'utcoffset',\n",
    " 'utctimetuple',\n",
    " 'weekday',\n",
    " 'year']\n",
    "# time_struct \n",
    " 'count',\n",
    " 'index',\n",
    " 'n_fields',\n",
    " 'n_sequence_fields',\n",
    " 'n_unnamed_fields',\n",
    " 'tm_gmtoff',\n",
    " 'tm_hour',\n",
    " 'tm_isdst',\n",
    " 'tm_mday',\n",
    " 'tm_min',\n",
    " 'tm_mon',\n",
    " 'tm_sec',\n",
    " 'tm_wday',\n",
    " 'tm_yday',\n",
    " 'tm_year',\n",
    " 'tm_zone'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55d6f3-c3f9-4ff5-a8a7-961b3fdb54d4",
   "metadata": {},
   "source": [
    "# namespace\n",
    "'triton-nick'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67403ac4-3510-4882-8bc4-05595785cbff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_single_query(timestamp_tuples, \n",
    "                     query, \n",
    "                     step=\"120s\", \n",
    "                     namespace='triton',\n",
    "                     deduplicate=False,\n",
    "                     prom=PrometheusConnect(url=\"http://lsdataitb.fnal.gov:9009/prometheus\", disable_ssl=True)):\n",
    "    \n",
    "    results = []\n",
    "    errors = []\n",
    "    print(f\"Running Query: {query}\")\n",
    "    for st, et in track(timestamp_tuples, description = f\"Retrieving - \"):\n",
    "        test_inp = prom.custom_query_range(\n",
    "            query=query,\n",
    "            start_time=parse_datetime(st),\n",
    "            end_time=parse_datetime(et),\n",
    "            step=step\n",
    "        )\n",
    "        # Queries are converted to a pandas dataframe\n",
    "        try:\n",
    "            df = MetricRangeDataFrame(test_inp)\n",
    "            results.append(df)\n",
    "        except:\n",
    "            if isinstance(test_inp, list) and len(test_inp) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                errors.append(test_inp)\n",
    "    # Dataframes are concatenated together along the time (index value) axis\n",
    "    if len(results) > 0:\n",
    "        temp = pd.concat(results, axis=0)\n",
    "        # Remove duplicates\n",
    "        ## return temp, errors\n",
    "        if deduplicate:\n",
    "            temp = temp[~temp.index.duplicated(keep='first')]\n",
    "        return temp, errors\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "def get_all_queries_v2(timestamp_tuples, step=\"120s\", namespace='triton', \n",
    "                       prom=PrometheusConnect(url=\"http://lsdataitb.fnal.gov:9009/prometheus\", disable_ssl=True)):\n",
    "    # FIXME: Refactor this into a single-query function (plus, a model_version single-query function, and another for the GPU stats)\n",
    "    # Then make calls to those function(s) with a wrapping function containing these queries.\n",
    "    rs = \"\"\n",
    "    rsm = \"\"\n",
    "    if isinstance(namespace, str):\n",
    "        rs = \"{namespace='\"+namespace+\"'}\"\n",
    "        rsm = \",namespace='\"+namespace+\"'\"\n",
    "    # A dictionary for our results\n",
    "    results = {}\n",
    "    errors = {}\n",
    "    # Tuples of the queries we'll make, for debugging and info\n",
    "    queries = []\n",
    "    \n",
    "    # Some queries are best created after understanding which unique models+version have been run in the triton servers\n",
    "    # and which GPU instances have been active. These are then used to formulate model/version-specific and GPU-specific stats\n",
    "    unique_model_versions, inactive_model_versions = find_active_models(timestamp_tuples, namespace=namespace, prom=prom)\n",
    "    unique_gpu_instances = None\n",
    "    \n",
    "    #Basic queries. Some of them are used as proxies to figure out the unqique queries to make later, like the \"gpu_tensor_util\" below\n",
    "    for key, query in {\n",
    "        \"num_instances\": \"count((sum by(pod) (delta(nv_inference_request_success\"+rs+\"[\"+step+\"]))) > 0)\",\n",
    "        \"inf_rate_net\":\"sum (rate(nv_inference_count\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_rate_bypod\":\"sum by(pod) (rate(nv_inference_count\"+rs+\"[\"+step+\"]))\",\n",
    "        ##\"inf_rate\":\"sum by(model, version, pod) (rate(nv_inference_count\"+rs+\"[\"+step+\"]))\",\n",
    "        #\"inf_cache_hit_rate\":\"sum by(model, version, pod) (rate(nv_cache_num_hits_per_model\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_reqs_net\":\"sum(rate(nv_inference_request_success\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_reqs_bypod\":\"sum by(pod) (rate(nv_inference_request_success\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_reqs\":\"sum by(model, version, pod) (rate(nv_inference_request_success\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_req_dur_net\": \"avg (delta(nv_inference_request_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_que_dur_net\": \"avg (delta(nv_inference_queue_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_inp_dur_net\": \"avg (delta(nv_inference_compute_input_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_inf_dur_net\": \"avg (delta(nv_inference_compute_infer_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_out_dur_net\": \"avg (delta(nv_inference_compute_output_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        ##\"inf_req_dur\": \"avg by(model, version, pod) (delta(nv_inference_request_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        ##\"inf_que_dur\": \"avg by(model, version, pod) (delta(nv_inference_queue_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        ##\"inf_inp_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_input_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        ##\"inf_inf_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_infer_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        ##\"inf_out_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_output_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"gpu_tensor_util\": \"sum by(device,GPU_I_ID,instance) (avg_over_time (DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        \"gpu_dram_util\": \"sum by(device,GPU_I_ID,instance) (avg_over_time (DCGM_FI_PROF_DRAM_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        #\"inf_cache_hits\": \"avg by(model, version, pod) (delta(nv_cache_num_hits_per_model[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        }.items():\n",
    "        # Log the queries, as they're easier to parse after being resolved fully\n",
    "        queries.append((key, query))\n",
    "        # Dataframes are concatenated together along the time (index value) axis\n",
    "        results[key], errors[key] = run_single_query(timestamp_tuples, query, step=step, prom=prom)\n",
    "        \n",
    "        # If we've performed a query that stores model/version info and GPU instance info, respectively, we can \n",
    "        # Create a set of unique ones for the next two sets of queries\n",
    "        #if unique_model_versions is None and not isinstance(results[key], list) and hasattr(results[key], \"model\") and hasattr(results[key], \"version\"):\n",
    "        #    unique_model_versions = set((results[key].model+\"/\"+results[key].version).values)\n",
    "        # At the EAF, the device ('nvidiaX' where X is 0...4 for example), GPU instance ID (enumeration)\n",
    "        # and the instance (IP address of host machine) are sufficient to make a unique identifier\n",
    "        if unique_gpu_instances is None and not isinstance(results[key], list) and hasattr(results[key], \"GPU_I_ID\"):\n",
    "            unique_gpu_instances = set((results[key].device+\"/\"+results[key].GPU_I_ID+\"/\"+results[key].instance).values)\n",
    "    # Here we build the model-specific queries, getting both the number of unique number of Triton instances that served \n",
    "    # inference requests for this model, ad well as the inference rate of that model across all Triton instances active per time step\n",
    "    if unique_model_versions is not None:\n",
    "        model_queries = {\"num_instances_\"+model_version: \"count((sum by(pod) (delta(nv_inference_request_success{model='\"+\n",
    "                         model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"]))) > 0)\"\n",
    "                         for model_version in unique_model_versions}\n",
    "        model_queries.update(\n",
    "            {\"inf_rate_\"+model_version: \"sum (rate(nv_inference_count{model='\"+\n",
    "             model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"]))\"\n",
    "             for model_version in unique_model_versions})\n",
    "        for key, query in model_queries.items():\n",
    "            queries.append((key, query))\n",
    "            results[key], errors[key] = run_single_query(timestamp_tuples, query, step=step, prom=prom)\n",
    "            if results[key] is None:\n",
    "                # If somehow we got no results for this model query, remove it from the dictionary and avoid iterating over it later\n",
    "                try:\n",
    "                    results.pop(key)\n",
    "                    unique_model_versions.remove(key.replace(\"inf_rate_\", \"\").replace(\"num_instances_\", \"\"))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # Now we gather the GPU metrics. The two most interesting ones for us are the DCGM_FI_PROF_PIPE_TENSOR_ACTIVE and \n",
    "    # DCGM_FI_PROF_DRAM_ACTIVE. The former measures how much of the compute resources (the Tensor Cores) are active, on average, in a time period\n",
    "    # If the utilization is 50%, this could mean that the tensor cores for this GPU (slice) are 100% active for 50% of the time, 50% active for\n",
    "    # 100% of the time, or any combination of activity_percent * time_active_percent that gives that product.\n",
    "    if unique_gpu_instances is not None:\n",
    "        gpu_queries = {\"gpu_tensor_util_\"+str(mg): \"sum (avg_over_time(DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{\"+\n",
    "                       \"exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\"+\n",
    "                       \"device='\"+gpu_inst.split(\"/\")[0]+\"',GPU_I_ID='\"+gpu_inst.split(\"/\")[1]+\"',instance='\"+gpu_inst.split(\"/\")[2]+\"'}[\"+step+\"]))\" for mg, gpu_inst in enumerate(unique_gpu_instances)}\n",
    "        # An example of how additional labels can filter out non-matching queries, if we do \n",
    "        # DCGM_FI_PROF_DRAM_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\n",
    "        #                          device='nvidia2',GPU_I_ID='3',instance='110.4.29.45'}[120s]\n",
    "        # We'll only get metrics from that specific device, if it has a running instance with that IP, and a running GPU instance matching it\n",
    "        # In this case, for each timestep, it'll get a 'vector' of instantaenous measurements within 120s\n",
    "        # The avg_over_time function then measures the average over time of that 'vector' and produces a scalar result\n",
    "        # The scalar result may not be unique for a given timestamp, there can be other labels attached, and a final avg is taken over all\n",
    "        # of those\n",
    "        gpu_queries.update(\n",
    "            {\"gpu_dram_util_\"+str(mg): \"avg (avg_over_time(DCGM_FI_PROF_DRAM_ACTIVE{\"+\n",
    "             \"exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\"+\n",
    "            \"device='\"+gpu_inst.split(\"/\")[0]+\"',GPU_I_ID='\"+gpu_inst.split(\"/\")[1]+\"',instance='\"+gpu_inst.split(\"/\")[2]+\"'}[\"+step+\"]))\"\n",
    "             for mg, gpu_inst in enumerate(unique_gpu_instances)})\n",
    "        for key, query in gpu_queries.items():\n",
    "            queries.append((key, query))\n",
    "            results[key], errors[key] = run_single_query(timestamp_tuples, query, step=step, prom=prom)\n",
    "            if results[key] is None:\n",
    "                #print(f\"results empty for {key}\")\n",
    "                try:\n",
    "                    results.pop(key)\n",
    "                    unique_gpu_instances.remove(key.split(\"_util_\")[1])\n",
    "                except:\n",
    "                    pass\n",
    "    return results, errors, queries, unique_model_versions, unique_gpu_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539d7c3-36d5-4654-a74d-3c8e297f61ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prom_query_hash(query_result):\n",
    "    \"\"\"Return a string-key to hash the result of a query, based on the labels Prometheus attaches\"\"\"\n",
    "    metric_dict = query_result['metric']\n",
    "    key = \"\"\n",
    "    for k, v in metric_dict.items():\n",
    "        key += \"($)\" +  k + \"::\" + v\n",
    "    return key\n",
    "\n",
    "def prom_query_add(query_A, query_B):\n",
    "    result = {}\n",
    "    result['metric'] = copy.deepcopy(query_A['metric'])\n",
    "    hash_A = prom_query_hash(query_A)\n",
    "    hash_B = prom_query_hash(query_B)\n",
    "    assert hash_A == hash_B, f\"Incompatible metrics are being added: {query_A['metric']} |INCOMPATIBLE WITH| {query_B['metric']}\"\n",
    "    result['values'] = copy.deepcopy(query_A['values'])\n",
    "    result['values'] += copy.deepcopy(query_B['values'])\n",
    "    return result\n",
    "\n",
    "def single_query_splt(timestamp_tuples, \n",
    "                      query, \n",
    "                      step=\"120s\", \n",
    "                      namespace='triton',\n",
    "                      deduplicate=False,\n",
    "                      dataframe_mode=\"individual\", #\"unified\", \"individual\", \"naive\"\n",
    "                      prom=None):\n",
    "    if prom is None:\n",
    "        prom = PrometheusConnect(url=\"http://lsdataitb.fnal.gov:9009/prometheus\", disable_ssl=True)\n",
    "    results_dict = {}\n",
    "    errors = []\n",
    "    print(f\"Running Query: {query}\")\n",
    "    for st, et in track(timestamp_tuples, description = f\"Retrieving\"):\n",
    "        test_inp = prom.custom_query_range(\n",
    "            query=query,\n",
    "            start_time=parse_datetime(st),\n",
    "            end_time=parse_datetime(et),\n",
    "            step=step\n",
    "        )\n",
    "        for query_result in test_inp:\n",
    "            key = prom_query_hash(query_result)\n",
    "            if key not in results_dict:\n",
    "                results_dict[key] = query_result\n",
    "            else:\n",
    "                results_dict[key] = prom_query_add(results_dict[key], query_result)\n",
    "            \n",
    "    # Queries are converted to a pandas dataframe\n",
    "    if dataframe_mode.lower() == \"individual\":\n",
    "        results = []\n",
    "        for key in results_dict:\n",
    "            try:\n",
    "                df = MetricRangeDataFrame(results_dict[key])\n",
    "                if deduplicate:\n",
    "                    df = df[~df.index.duplicated(keep='first')]\n",
    "                results.append(df)\n",
    "            except:\n",
    "                errors.append({key: results_dict[key]})\n",
    "    elif dataframe_mode.lower() == \"bypass\":\n",
    "        results = list(results_dict.values())\n",
    "    else:\n",
    "        try:\n",
    "            df = MetricRangeDataFrame(list(results_dict.values()))\n",
    "            if deduplicate:\n",
    "                df = df[~df.index.duplicated(keep='first')]\n",
    "            results = [df]\n",
    "        except:\n",
    "            errors.append(results_dict)\n",
    "            \n",
    "    if len(results) > 0:\n",
    "        temp = results\n",
    "        return temp, errors\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def get_all_queries_v3(timestamp_tuples, step=\"120s\", granular_step=None, namespace='triton', deduplicate=False,\n",
    "                       prom=PrometheusConnect(url=\"http://lsdataitb.fnal.gov:9009/prometheus\", disable_ssl=True)):\n",
    "    rs = \"\"\n",
    "    rsm = \"\"\n",
    "    if isinstance(namespace, str):\n",
    "        rs = \"{namespace='\"+namespace+\"'}\"\n",
    "        rsm = \",namespace='\"+namespace+\"'\"\n",
    "    # A dictionary for our results\n",
    "    results = {}\n",
    "    errors = {}\n",
    "    # Tuples of the queries we'll make, for debugging and info\n",
    "    queries = []\n",
    "    \n",
    "    # Some queries are best created after understanding which unique models+version have been run in the triton servers\n",
    "    # and which GPU instances have been active. These are then used to formulate model/version-specific and GPU-specific stats\n",
    "    columns_step = granular_step if granular_step is not None else step\n",
    "    unique_model_versions, inactive_model_versions = find_active_models(timestamp_tuples, step=columns_step, namespace=namespace, prom=prom)\n",
    "    unique_gpu_instances = find_all_gpus(timestamp_tuples, step=columns_step, namespace=None, prom=prom) #different namespace entirely\n",
    "    \n",
    "    #Basic queries. Some of them are used as proxies to figure out the unqique queries to make later, like the \"gpu_tensor_util\" below\n",
    "    for key, query in {\n",
    "        \"num_instances\": \"count((sum by(pod) (delta(nv_inference_request_success\"+rs+\"[\"+step+\"]))) > 0)\",\n",
    "        \"inf_rate_net\":\"sum (rate(nv_inference_count\"+rs+\"[\"+step+\"]))\",\n",
    "        ##\"inf_rate_bypod\":\"sum by(pod) (rate(nv_inference_count\"+rs+\"[\"+step+\"]))\",\n",
    "        ##\"inf_rate\":\"sum by(model, version, pod) (rate(nv_inference_count\"+rs+\"[\"+step+\"]))\",\n",
    "        #\"inf_cache_hit_rate\":\"sum by(model, version, pod) (rate(nv_cache_num_hits_per_model\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_reqs_net\":\"sum(rate(nv_inference_request_success\"+rs+\"[\"+step+\"]))\",\n",
    "        ##\"inf_reqs_bypod\":\"sum by(pod) (rate(nv_inference_request_success\"+rs+\"[\"+step+\"]))\",\n",
    "        ##\"inf_reqs\":\"sum by(model, version, pod) (rate(nv_inference_request_success\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_req_dur_net\": \"avg (delta(nv_inference_request_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_que_dur_net\": \"avg (delta(nv_inference_queue_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_inp_dur_net\": \"avg (delta(nv_inference_compute_input_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_inf_dur_net\": \"avg (delta(nv_inference_compute_infer_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_out_dur_net\": \"avg (delta(nv_inference_compute_output_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        #\"inf_req_dur\": \"avg by(model, version, pod) (delta(nv_inference_request_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        #\"inf_que_dur\": \"avg by(model, version, pod) (delta(nv_inference_queue_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        #\"inf_inp_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_input_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        #\"inf_inf_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_infer_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        #\"inf_out_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_output_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        ##\"gpu_tensor_util\": \"sum by(device,GPU_I_ID,instance) (avg_over_time (DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        ##\"gpu_dram_util\": \"sum by(device,GPU_I_ID,instance) (avg_over_time (DCGM_FI_PROF_DRAM_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        #\"inf_cache_hits\": \"avg by(model, version, pod) (delta(nv_cache_num_hits_per_model[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        }.items():\n",
    "        # Log the queries, as they're easier to parse after being resolved fully\n",
    "        queries.append((key, query))\n",
    "        # Dataframes are concatenated together along the time (index value) axis\n",
    "        results[key], errors[key] = single_query_splt(timestamp_tuples, \n",
    "                                                      query, \n",
    "                                                      step=step, \n",
    "                                                      namespace=namespace,\n",
    "                                                      deduplicate=deduplicate,\n",
    "                                                      dataframe_mode=\"individual\", #\"unified\", \"individual\", \"naive\"\n",
    "                                                      prom=prom)\n",
    "        \n",
    "        # If we've performed a query that stores model/version info and GPU instance info, respectively, we can \n",
    "        # Create a set of unique ones for the next two sets of queries\n",
    "        #if unique_model_versions is None and not isinstance(results[key], list) and hasattr(results[key], \"model\") and hasattr(results[key], \"version\"):\n",
    "        #    unique_model_versions = set((results[key].model+\"/\"+results[key].version).values)\n",
    "        # At the EAF, the device ('nvidiaX' where X is 0...4 for example), GPU instance ID (enumeration)\n",
    "        # and the instance (IP address of host machine) are sufficient to make a unique identifier\n",
    "        if unique_gpu_instances is None and not isinstance(results[key], list) and hasattr(results[key], \"GPU_I_ID\"):\n",
    "            unique_gpu_instances = set((results[key].device+\"/\"+results[key].GPU_I_ID+\"/\"+results[key].instance).values)\n",
    "    # Here we build the model-specific queries, getting both the number of unique number of Triton instances that served \n",
    "    # inference requests for this model, ad well as the inference rate of that model across all Triton instances active per time step\n",
    "    if unique_model_versions is not None:\n",
    "        model_queries = {\"num_instances_\"+model_version: \"count((sum by(pod) (delta(nv_inference_request_success{model='\"+\n",
    "                         model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"]))) > 0)\"\n",
    "                         for model_version in unique_model_versions}\n",
    "        model_queries.update(\n",
    "            {\"inf_rate_\"+model_version: \"sum (rate(nv_inference_count{model='\"+\n",
    "             model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"]))\"\n",
    "             for model_version in unique_model_versions})\n",
    "        model_queries.update(\n",
    "            {\"inf_req_dur_\"+model_version: \"avg (delta(nv_inference_request_duration_us{model='\"+\n",
    "             model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"])/\"+\n",
    "             \"(0.001+delta(nv_inference_request_success{model='\"+model_version.split(\"/\")[0]+\n",
    "             \"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"])))\"\n",
    "             for model_version in unique_model_versions})\n",
    "        model_queries.update(\n",
    "            {\"inf_que_dur_\"+model_version: \"avg (delta(nv_inference_queue_duration_us{model='\"+\n",
    "             model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"])/\"+\n",
    "             \"(0.001+delta(nv_inference_request_success{model='\"+model_version.split(\"/\")[0]+\n",
    "             \"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"])))\"\n",
    "             for model_version in unique_model_versions})\n",
    "        model_queries.update(\n",
    "            {\"inf_inp_dur_\"+model_version: \"avg (delta(nv_inference_compute_input_duration_us{model='\"+\n",
    "             model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"])/\"+\n",
    "             \"(0.001+delta(nv_inference_request_success{model='\"+model_version.split(\"/\")[0]+\n",
    "             \"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"])))\"\n",
    "             for model_version in unique_model_versions})\n",
    "        model_queries.update(\n",
    "            {\"inf_inf_dur_\"+model_version: \"avg (delta(nv_inference_compute_infer_duration_us{model='\"+\n",
    "             model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"])/\"+\n",
    "             \"(0.001+delta(nv_inference_request_success{model='\"+model_version.split(\"/\")[0]+\n",
    "             \"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"])))\"\n",
    "             for model_version in unique_model_versions})\n",
    "        model_queries.update(\n",
    "            {\"inf_out_dur_\"+model_version: \"avg (delta(nv_inference_compute_output_duration_us{model='\"+\n",
    "             model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"])/\"+\n",
    "             \"(0.001+delta(nv_inference_request_success{model='\"+model_version.split(\"/\")[0]+\n",
    "             \"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"])))\"\n",
    "             for model_version in unique_model_versions})\n",
    "        #\"inf_req_dur_net\": \"avg (delta(nv_inference_request_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        #\"inf_que_dur_net\": \"avg (delta(nv_inference_queue_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        #\"inf_inp_dur_net\": \"avg (delta(nv_inference_compute_input_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        #\"inf_inf_dur_net\": \"avg (delta(nv_inference_compute_infer_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        #\"inf_out_dur_net\": \"avg (delta(nv_inference_compute_output_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        for key, query in model_queries.items():\n",
    "            queries.append((key, query))\n",
    "            results[key], errors[key] = single_query_splt(timestamp_tuples, \n",
    "                                                          query, \n",
    "                                                          step=step, \n",
    "                                                          namespace=namespace,\n",
    "                                                          deduplicate=deduplicate,\n",
    "                                                          dataframe_mode=\"individual\", #\"unified\", \"individual\", \"naive\"\n",
    "                                                          prom=prom)\n",
    "            if results[key] is None:\n",
    "                # If somehow we got no results for this model query, remove it from the dictionary and avoid iterating over it later\n",
    "                try:\n",
    "                    results.pop(key)\n",
    "                    unique_model_versions.remove(key.replace(\"inf_rate_\", \"\").replace(\"num_instances_\", \"\"))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # Now we gather the GPU metrics. The two most interesting ones for us are the DCGM_FI_PROF_PIPE_TENSOR_ACTIVE and \n",
    "    # DCGM_FI_PROF_DRAM_ACTIVE. The former measures how much of the compute resources (the Tensor Cores) are active, on average, in a time period\n",
    "    # If the utilization is 50%, this could mean that the tensor cores for this GPU (slice) are 100% active for 50% of the time, 50% active for\n",
    "    # 100% of the time, or any combination of activity_percent * time_active_percent that gives that product.\n",
    "    if unique_gpu_instances is not None:\n",
    "        gpu_queries = {\"gpu_tensor_util_\"+str(mg): \"sum (avg_over_time(DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{\"+\n",
    "                       \"exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\"+\n",
    "                       \"device='\"+gpu_inst.split(\"/\")[0]+\"',GPU_I_ID='\"+gpu_inst.split(\"/\")[1]+\"',instance='\"+gpu_inst.split(\"/\")[2]+\"'}[\"+step+\"]))\" for mg, gpu_inst in enumerate(unique_gpu_instances)}\n",
    "        # An example of how additional labels can filter out non-matching queries, if we do \n",
    "        # DCGM_FI_PROF_DRAM_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\n",
    "        #                          device='nvidia2',GPU_I_ID='3',instance='110.4.29.45'}[120s]\n",
    "        # We'll only get metrics from that specific device, if it has a running instance with that IP, and a running GPU instance matching it\n",
    "        # In this case, for each timestep, it'll get a 'vector' of instantaenous measurements within 120s\n",
    "        # The avg_over_time function then measures the average over time of that 'vector' and produces a scalar result\n",
    "        # The scalar result may not be unique for a given timestamp, there can be other labels attached, and a final avg is taken over all\n",
    "        # of those\n",
    "        gpu_queries.update(\n",
    "            {\"gpu_dram_util_\"+str(mg): \"avg (avg_over_time(DCGM_FI_PROF_DRAM_ACTIVE{\"+\n",
    "             \"exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\"+\n",
    "            \"device='\"+gpu_inst.split(\"/\")[0]+\"',GPU_I_ID='\"+gpu_inst.split(\"/\")[1]+\"',instance='\"+gpu_inst.split(\"/\")[2]+\"'}[\"+step+\"]))\"\n",
    "             for mg, gpu_inst in enumerate(unique_gpu_instances)})\n",
    "        for key, query in gpu_queries.items():\n",
    "            queries.append((key, query))\n",
    "            results[key], errors[key] = single_query_splt(timestamp_tuples, \n",
    "                                                          query, \n",
    "                                                          step=step, \n",
    "                                                          namespace=namespace,\n",
    "                                                          deduplicate=deduplicate,\n",
    "                                                          dataframe_mode=\"individual\", #\"unified\", \"individual\", \"naive\"\n",
    "                                                          prom=prom)\n",
    "            if results[key] is None:\n",
    "                #print(f\"results empty for {key}\")\n",
    "                try:\n",
    "                    results.pop(key)\n",
    "                    unique_gpu_instances.remove(key.split(\"_util_\")[1])\n",
    "                except:\n",
    "                    pass\n",
    "    return results, errors, queries, unique_model_versions, unique_gpu_instances\n",
    "\n",
    "\n",
    "def find_active_models(timestamp_tuples, step=\"120s\", namespace=None, prom=None):\n",
    "    #st = timestamp_tuples[0][0]\n",
    "    #et = timestamp_tuples[-1][1]\n",
    "    #step = \n",
    "    results = single_query_splt(\n",
    "        [(timestamp_tuples[0][0], timestamp_tuples[-1][1])],\n",
    "        \"sum by(model, version) (rate(nv_inference_count[\"+step+\"]))\",\n",
    "        step=step,\n",
    "        namespace=namespace,\n",
    "        dataframe_mode=\"bypass\",\n",
    "        deduplicate=False,\n",
    "        prom=prom\n",
    "    )[0] #Only need results\n",
    "    active_models = []\n",
    "    inactive_models = []\n",
    "    for mv in results:\n",
    "        vals = mv['values']\n",
    "        total = sum([float(val[1]) for val in vals])\n",
    "        if total > 0:\n",
    "            active_models.append(mv['metric']['model'] + \"/\" + mv['metric']['version'])\n",
    "            #print(total, mv['metric'])\n",
    "        else:\n",
    "            inactive_models.append(mv['metric']['model'] + \"/\" + mv['metric']['version'])\n",
    "            #print(\"0 rate: \", mv['metric'])\n",
    "    return active_models, inactive_models\n",
    "\n",
    "def find_all_gpus(timestamp_tuples, step=\"120s\", namespace=None, prom=None):\n",
    "    results = single_query_splt(\n",
    "        [(timestamp_tuples[0][0], timestamp_tuples[-1][1])],\n",
    "        \"sum by(device, GPU_I_ID, instance) (avg_over_time (DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        step=step,\n",
    "        namespace=namespace,\n",
    "        dataframe_mode=\"bypass\",\n",
    "        deduplicate=False,\n",
    "        prom=prom\n",
    "    )[0] #Only need results\n",
    "    devices = []\n",
    "    for mv in results:\n",
    "        vals = mv['values']\n",
    "        devices.append(mv['metric']['device'] + \"/\" + mv['metric']['GPU_I_ID'] + \"/\" + mv['metric']['instance'])\n",
    "    return devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072d6cc-3b7d-49fb-ada3-52b253d0d19d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_v3, errors_v3, queries_v3, unique_model_versions_v3, unique_gpu_instances_v3 = get_all_queries_v3(\n",
    "    [(\"2023-02-28 12:00:00\", \"2023-03-12 12:00:00\"),\n",
    "     (\"2023-03-12 12:00:00\", \"2023-03-24 12:00:00\"),\n",
    "     (\"2023-03-24 12:00:00\", \"2023-04-06 12:00:00\"),\n",
    "     (\"2023-04-06 12:00:00\", \"2023-04-18 12:00:00\"),\n",
    "     (\"2023-04-18 12:00:00\", \"2023-04-30 16:00:00\"),\n",
    "    ],\n",
    "    namespace=None,\n",
    "    deduplicate=True,\n",
    "    step=\"120s\",\n",
    "    granular_step=\"1d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33985df-178d-4b84-963c-4fbe650a4ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_gpu_instances_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed83e48-8742-4015-8682-4fad63810529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_v3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3da81-4f18-4513-96eb-8a2a99911482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_results_to_df_v2(results, step, unique_model_versions=None, unique_gpu_instances=None, add_model_stats=True, add_gpu_stats=False):\n",
    "    # This iteratively walks through some of the dataframes that are compatible and aggregates results into a \n",
    "    # unified dataframe. In each dataframe, the join call, in combination with how='left', means that results are broadcast\n",
    "    # and filled with NaN wherever results may be missing from the second of the two dataframes.\n",
    "    # For this reason, the 'inf_rate_net' which should have a valid value for all timestamps is used as the base.\n",
    "    ##idx = pd.period_range(min(df.date), max(df.date))\n",
    "    ##...: results.reindex(idx, fill_value=0)\n",
    "    min_dates = []\n",
    "    max_dates = []\n",
    "    for k, vlist in results.items():\n",
    "        for v in vlist:\n",
    "            min_dates.append(min(v.index.values))\n",
    "            max_dates.append(max(v.index.values))\n",
    "    min_date = min(min_dates)\n",
    "    max_date = max(max_dates)\n",
    "    new_index = pd.date_range(min_date, max_date, freq=step)\n",
    "    ret = None\n",
    "    for k, vlist in results.items():\n",
    "        it = 0\n",
    "        if len(vlist) > 1:\n",
    "            print(f\"Unable to add results column {k} due to multiple un-keyed results\")\n",
    "        else:\n",
    "            # Make this an interable dict being returned from the split-query mode, then the above exception can be removed\n",
    "            for v in vlist:\n",
    "                it += 1\n",
    "                try:\n",
    "                    tmp = v.reindex(new_index, fill_value=0)\n",
    "                except:\n",
    "                    print(f\"failure to reindex {k} {it} --> {v.columns}\")\n",
    "                    return new_index, v\n",
    "            if ret is None:\n",
    "                ret = tmp.rename(columns={\"value\": k})\n",
    "            else:\n",
    "                assert np.all(ret.index.values == tmp.index.values), \"Mismatched Time Indices Detected\"\n",
    "                ret.loc[:, k] = tmp.value\n",
    "    return ret\n",
    "        \n",
    "checkcheck = convert_results_to_df_v2({k: v for k, v in results_v3.items()}, step=\"120s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336fe03-c9ba-4c86-be5c-6fec7b6fc28d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc = checkcheck\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36062e2-2ef4-4880-b1a0-3424e1588309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc.to_csv(\"test_pn_demos.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1907db85-9767-4194-9496-7f56b94624d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queues = cc.loc[:, ['inf_que_dur_emj_gnn_aligned/1',\n",
    "             'inf_que_dur_pn_demo/1',\n",
    "             'inf_que_dur_pn_demo_bkg1/1',\n",
    "             'inf_que_dur_pn_demo_bkg2/1',\n",
    "             'inf_que_dur_pn_demo_bkg3/1',\n",
    "             'inf_que_dur_reconstruction_bdt_xgb/1',\n",
    "             'inf_que_dur_svj_tch_gnn/1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7976850-4c26-48e4-9521-96a6b63fe754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_ms = queues.max(axis=1)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6adedeb-2f99-4283-820c-f92a114bad88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(cc['inf_rate_net']/cc['num_instances'], max_ms.values)\n",
    "plt.ylim(0, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f7fce-68d0-4212-ad03-e53b7050b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def the_rest():\n",
    "    new_index = pd.period_range(min_date, max_date)\n",
    "    for key in [\"inf_rate_net\", \"inf_reqs_net\", \"inf_req_dur_net\", \"inf_que_dur_net\", \"inf_inp_dur_net\", \"inf_inf_dur_net\", \"inf_out_dur_net\"]:\n",
    "        results[key].reindex(new_index, fill_value=0)\n",
    "    i0 = results[\"inf_rate_net\"].join(results[\"num_instances\"],\n",
    "                                      how=\"left\", \n",
    "                                      rsuffix=\"_num_instances\",\n",
    "                                     )\n",
    "    # We use the rsuffix and lsuffix to convert column names from 'value' to one that is understandable/parseable later on. \n",
    "    i0 = i0.join(results[\"inf_reqs_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_reqs_net\")\n",
    "    i0 = i0.join(results[\"inf_req_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_req_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_que_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_que_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_inp_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_inp_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_inf_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_inf_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_out_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_out_dur_net\")\n",
    "    \n",
    "    #Add the model metrics, using some suffix parsing to make it into num_instances_X or rate_X where X is the model name\n",
    "    if add_model_stats:\n",
    "        for model in track(unique_model_versions, description=\"Adding Model Stats\"):\n",
    "            itemp = results[\"inf_rate_\" + model].join(results[\"num_instances_\" + model],\n",
    "                                                      how=\"left\",\n",
    "                                                      rsuffix=\"_num_instances_\"+model.split(\"/\")[0],\n",
    "                                                      lsuffix=\"_rate_\"+model.split(\"/\")[0],\n",
    "                                                     )\n",
    "            i0 = i0.join(itemp, how=\"left\")\n",
    "        \n",
    "    #Add the GPU Instance metrics, including GPU instance enumeration\n",
    "    if add_gpu_stats:\n",
    "        for mg, gpu in track(enumerate(unique_gpu_instances), description=\"Adding GPU Stats\"):\n",
    "            results[\"gpu_tensor_util_\" + str(mg)].fillna(0, inplace=True)\n",
    "            results[\"gpu_dram_util_\" + str(mg)].fillna(0, inplace=True)\n",
    "            itemp = results[\"gpu_tensor_util_\" + str(mg)].join(results[\"gpu_dram_util_\" + str(mg)],\n",
    "                                                      how=\"left\",\n",
    "                                                      rsuffix=\"_gpu_dram_util_\"+str(mg),\n",
    "                                                      lsuffix=\"_gpu_tensor_util_\"+str(mg),\n",
    "                                                     )\n",
    "            i0 = i0.join(itemp, how=\"left\")\n",
    "\n",
    "    #Get rid of the \"value\" in column names, and fill NaN values with 0 everywhere\n",
    "    i0.rename(columns={\"value\": \"rate\"}, inplace=True)\n",
    "    i0.rename(columns={col:col[6:] for col in i0.columns if col.startswith(\"value_\")}, inplace=True)\n",
    "    i0.fillna(0, inplace=True)\n",
    "    \n",
    "    # Aggregate some stats for models\n",
    "    # The summed rate and total inference rate should match, otherwise we've double-counted something\n",
    "    # The summed instances may NOT match: if a model is active on 5 of 10 servers in a timestep, and another is active on 7 of 10\n",
    "    # Then there will be '12' active instances in that timestep, net. This number divided by the net_instances\n",
    "    # Therefore gives a measure of the 'average' model concurrency in a timestep. 10 net_instances and 70 summed_intstances\n",
    "    # would indicate each instances was serving 7 models at some point in that timestep (but this is a lossy gathering of information,\n",
    "    # 6 models could do one inference request while the last model is responsible for all of the remainder of thousands of requests.\n",
    "    valid_model_keys = [col for col in i0.columns if col.startswith(\"rate_\") and col.replace(\"rate_\", \"num_instances_\") in i0.columns]\n",
    "    i0[\"summed_rate\"] = sum([i0[col] for col in valid_model_keys])\n",
    "    i0[\"summed_instances\"] = sum([i0[col.replace(\"rate_\", \"num_instances_\")] for col in valid_model_keys])\n",
    "    \n",
    "    # Aggregate some stats for GPU instances\n",
    "    valid_gpu_keys = [col for col in i0.columns if col.startswith(\"gpu_tensor_util\") and col.replace(\"tensor\", \"dram\") in i0.columns]\n",
    "    i0[\"summed_gpu_tensor_util\"] = sum([i0[col] for col in valid_gpu_keys])\n",
    "    i0[\"summed_gpu_dram_util\"] = sum([i0[col.replace(\"tensor\", \"dram\")] for col in valid_model_keys])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0ddf3-6b9c-4302-a68a-6903b5bb53e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test2 = single_query_splt([     \n",
    "     (\"2023-02-17 at 00:00:00 MDT\", \"2023-02-20 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-20 at 00:00:00 MDT\", \"2023-02-23 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-23 at 00:00:00 MDT\", \"2023-02-26 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-26 at 00:00:00 MDT\", \"2023-03-01 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-01 at 00:00:00 MDT\", \"2023-03-04 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-04 at 00:00:00 MDT\", \"2023-03-07 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-07 at 00:00:00 MDT\", \"2023-03-10 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-10 at 00:00:00 MDT\", \"2023-03-13 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-13 at 00:00:00 MDT\", \"2023-03-16 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-16 at 00:00:00 MDT\", \"2023-03-19 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-19 at 00:00:00 MDT\", \"2023-03-22 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-22 at 00:00:00 MDT\", \"2023-03-25 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-25 at 00:00:00 MDT\", \"2023-03-28 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-28 at 00:00:00 MDT\", \"2023-03-31 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-31 at 00:00:00 MDT\", \"2023-04-03 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-03 at 00:00:00 MDT\", \"2023-04-06 at 00:00:00 MDT\"),  \n",
    "     (\"2023-04-06 at 00:00:00 MDT\", \"2023-04-09 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-09 at 00:00:00 MDT\", \"2023-04-12 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-12 at 00:00:00 MDT\", \"2023-04-15 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-15 at 00:00:00 MDT\", \"2023-04-18 at 00:00:00 MDT\"),\n",
    "    ], \n",
    "    \"sum by(model, version) (rate(nv_inference_count[120s]))\",\n",
    "    step=\"120s\",\n",
    "    namespace=None,\n",
    "    dataframe_mode=\"individual\",\n",
    "    deduplicate=False\n",
    ")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db44e87-927f-493d-8400-e22d27ad546f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2458751-ec8c-47d8-94c0-59cae033d538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in test2[0]:\n",
    "    print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918b207-f203-45c2-ac45-11808286153c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test4 = run_single_query([     \n",
    "     (\"2023-02-17 at 00:00:00 MDT\", \"2023-02-20 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-20 at 00:00:00 MDT\", \"2023-02-23 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-23 at 00:00:00 MDT\", \"2023-02-26 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-26 at 00:00:00 MDT\", \"2023-03-01 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-01 at 00:00:00 MDT\", \"2023-03-04 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-04 at 00:00:00 MDT\", \"2023-03-07 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-07 at 00:00:00 MDT\", \"2023-03-10 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-10 at 00:00:00 MDT\", \"2023-03-13 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-13 at 00:00:00 MDT\", \"2023-03-16 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-16 at 00:00:00 MDT\", \"2023-03-19 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-19 at 00:00:00 MDT\", \"2023-03-22 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-22 at 00:00:00 MDT\", \"2023-03-25 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-25 at 00:00:00 MDT\", \"2023-03-28 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-28 at 00:00:00 MDT\", \"2023-03-31 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-31 at 00:00:00 MDT\", \"2023-04-03 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-03 at 00:00:00 MDT\", \"2023-04-06 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-06 at 00:00:00 MDT\", \"2023-04-09 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-09 at 00:00:00 MDT\", \"2023-04-12 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-12 at 00:00:00 MDT\", \"2023-04-15 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-15 at 00:00:00 MDT\", \"2023-04-18 at 00:00:00 MDT\"),\n",
    "    ], \n",
    "    \"sum by(model, version) (rate(nv_inference_count[120s]))\",\n",
    "    step=\"120s\",\n",
    "    namespace=None,\n",
    "    deduplicate=False\n",
    ")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d1457-7dd0-481b-be33-52b031d6922f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in test4[0]:\n",
    "    print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091b7bb-76bf-424e-86a6-13a6c518b589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subresults = 0\n",
    "for time_slice in test:\n",
    "    for it in time_slice:\n",
    "        for it2 in it:\n",
    "            subresults += 1\n",
    "print(subresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a5ad83-44c8-42fe-af7d-f46c90c332f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_results = {}\n",
    "for time_slice in test:\n",
    "    print(\"========\")\n",
    "    for it in time_slice:\n",
    "        print(type(it))\n",
    "        for it2 in it:\n",
    "            key = prom_query_hash(it2)\n",
    "            if key not in new_results:\n",
    "                new_results[key] = it2\n",
    "            else:\n",
    "                new_results[key] = prom_query_add(new_results[key], it2)\n",
    "       # print(prom_query_hash(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21c0dd-d0e2-4f96-ab1e-156370c63e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in new_results.keys():\n",
    "    tmp = k.split(\"($)\")\n",
    "    if len(tmp) != 4:\n",
    "        print(k, len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef9828-4180-4ccf-a68a-6ed282d0b07d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in new_results['($)model::deeptau_ensemble($)version::1']['values']:\n",
    "    if x[1] != '0':\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32015223-f118-4cf3-8d86-72cc2efbd2e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_models = dict()\n",
    "for key in new_results:\n",
    "    mk = key.split(\"($)\")\n",
    "    key = mk[0] + mk[2]\n",
    "    if key not in count_models:\n",
    "        count_models[key] = 1\n",
    "    else:\n",
    "        count_models[key] += 1\n",
    "print(count_models.values())\n",
    "print(list(count_models.keys())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4d9ad1-eb71-4122-b31b-2ebb05591291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(test[0]), len(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a0dcf-88bd-4864-936a-bc30915017ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in test[0][0]:\n",
    "    #print(type(x), x.keys(), x['metric'])\n",
    "    print(type(x), x.keys(), type(x['values']), x['values'][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbc6a9-3dbc-4fb7-ac73-582a09dc1d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_v2, errors_v2, queries_v2, unique_model_versions_v2, unique_gpu_instances_v2 = get_all_queries_v2(\n",
    "    [(\"2023-03-22 at 00:00:00 MDT\", \"2023-03-25 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-25 at 00:00:00 MDT\", \"2023-03-28 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-28 at 00:00:00 MDT\", \"2023-03-31 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-31 at 00:00:00 MDT\", \"2023-04-02 at 00:00:00 MDT\"),\n",
    "    ],\n",
    "    namespace=None,\n",
    "    step=\"120s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dbc21a-01ee-430c-b6ee-4e9e1f40a340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(results_v2['inf_rate_net'].index), len(set(results_v2['inf_rate_net'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eea5e7-f10b-43b9-a915-cba5feae0b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_v2, errors_v2, queries_v2, unique_model_versions_v2, unique_gpu_instances_v2 = get_all_queries_v2(\n",
    "    [#(\"2023-01-30 at 00:00:00 MDT\", \"2023-02-02 at 00:00:00 MDT\"),\n",
    "     #(\"2023-02-02 at 00:00:00 MDT\", \"2023-02-05 at 00:00:00 MDT\"),\n",
    "     #(\"2023-02-05 at 00:00:00 MDT\", \"2023-02-08 at 00:00:00 MDT\"),\n",
    "     #(\"2023-02-08 at 00:00:00 MDT\", \"2023-02-11 at 00:00:00 MDT\"),\n",
    "     #(\"2023-02-11 at 00:00:00 MDT\", \"2023-02-14 at 00:00:00 MDT\"),\n",
    "     #(\"2023-02-14 at 00:00:00 MDT\", \"2023-02-17 at 00:00:00 MDT\"),\n",
    "        \n",
    "     (\"2023-02-17 at 00:00:00 MDT\", \"2023-02-20 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-20 at 00:00:00 MDT\", \"2023-02-23 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-23 at 00:00:00 MDT\", \"2023-02-26 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-26 at 00:00:00 MDT\", \"2023-03-01 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-01 at 00:00:00 MDT\", \"2023-03-04 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-04 at 00:00:00 MDT\", \"2023-03-07 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-07 at 00:00:00 MDT\", \"2023-03-10 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-10 at 00:00:00 MDT\", \"2023-03-13 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-13 at 00:00:00 MDT\", \"2023-03-16 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-16 at 00:00:00 MDT\", \"2023-03-19 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-19 at 00:00:00 MDT\", \"2023-03-22 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-22 at 00:00:00 MDT\", \"2023-03-25 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-25 at 00:00:00 MDT\", \"2023-03-28 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-28 at 00:00:00 MDT\", \"2023-03-31 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-31 at 00:00:00 MDT\", \"2023-04-03 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-03 at 00:00:00 MDT\", \"2023-04-06 at 00:00:00 MDT\"),\n",
    "        \n",
    "     (\"2023-04-06 at 00:00:00 MDT\", \"2023-04-09 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-09 at 00:00:00 MDT\", \"2023-04-12 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-12 at 00:00:00 MDT\", \"2023-04-15 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-15 at 00:00:00 MDT\", \"2023-04-18 at 00:00:00 MDT\"),\n",
    "    ],\n",
    "    namespace=None,\n",
    "    step=\"120s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61e024-f31c-46e2-bf5f-9f4cd6fc54a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = run_single_query(\n",
    "    [     \n",
    "        (\"2023-02-17 at 00:00:00 MDT\", \"2023-02-20 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-20 at 00:00:00 MDT\", \"2023-02-23 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-23 at 00:00:00 MDT\", \"2023-02-26 at 00:00:00 MDT\"),\n",
    "     (\"2023-02-26 at 00:00:00 MDT\", \"2023-03-01 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-01 at 00:00:00 MDT\", \"2023-03-04 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-04 at 00:00:00 MDT\", \"2023-03-07 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-07 at 00:00:00 MDT\", \"2023-03-10 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-10 at 00:00:00 MDT\", \"2023-03-13 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-13 at 00:00:00 MDT\", \"2023-03-16 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-16 at 00:00:00 MDT\", \"2023-03-19 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-19 at 00:00:00 MDT\", \"2023-03-22 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-22 at 00:00:00 MDT\", \"2023-03-25 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-25 at 00:00:00 MDT\", \"2023-03-28 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-28 at 00:00:00 MDT\", \"2023-03-31 at 00:00:00 MDT\"),\n",
    "     (\"2023-03-31 at 00:00:00 MDT\", \"2023-04-03 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-03 at 00:00:00 MDT\", \"2023-04-06 at 00:00:00 MDT\"),\n",
    "        \n",
    "     (\"2023-04-06 at 00:00:00 MDT\", \"2023-04-09 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-09 at 00:00:00 MDT\", \"2023-04-12 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-12 at 00:00:00 MDT\", \"2023-04-15 at 00:00:00 MDT\"),\n",
    "     (\"2023-04-15 at 00:00:00 MDT\", \"2023-04-18 at 00:00:00 MDT\"),\n",
    "    ], \n",
    "    \"sum by(model, version, pod) (rate(nv_inference_count[120s]))\",\n",
    "    step=\"120s\",\n",
    "    namespace=None,\n",
    "    deduplicate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70316fca-fd7d-4bed-9260-ad6d1d379767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07bf80-9fad-4523-a097-7543430c315c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91015d0-544c-4753-96a1-6153d4b8fa7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(results_v2[\"num_instances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a67dbe-f148-4ec7-91e3-864015f9c059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_v2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfae8d4-3507-4a5a-b10d-8467392f76e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_v2['inf_rate'][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4c10d-724f-4ccd-bba6-2d4a1eedaf07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, v in results_v2.items():\n",
    "    if not isinstance(v, pd.core.frame.DataFrame):\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94bee51-3f95-41db-b1a1-7e50df63294b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_model_versions_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f2046-6e41-4340-b8cc-94ebfd43a5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function for getting queries of many GPU and Triton server metrics. Inputs are a list of timestamp tuples,\n",
    "# which can be parsed by the prometheus_api_client.utils.parse_datetime function. This can understand timestamps formatted like\n",
    "# \"2023-03-30 at 16:00:00 MDT\"\n",
    "# The step is the 'time-window' over which each query will be divided. This should be ~4x as long as the longest frequency for metric-gather\n",
    "def get_all_queries(timestamp_tuples, step, namespace='triton'):\n",
    "    # FIXME: Refactor this into a single-query function (plus, a model_version single-query function, and another for the GPU stats)\n",
    "    # Then make calls to those function(s) with a wrapping function containing these queries.\n",
    "    rs = \"\"\n",
    "    rsm = \"\"\n",
    "    if isinstance(namespace, str):\n",
    "        rs = \"{namespace='\"+namespace+\"'}\"\n",
    "        rsm = \",namespace='\"+namespace+\"'\"\n",
    "    # A dictionary for our results\n",
    "    results = {}\n",
    "    # Tuples of the queries we'll make, for debugging and info\n",
    "    queries = []\n",
    "    \n",
    "    # Some queries are best created after understanding which unique models+version have been run in the triton servers\n",
    "    # and which GPU instances have been active. These are then used to formulate model/version-specific and GPU-specific stats\n",
    "    unique_model_versions = None\n",
    "    unique_gpu_instances = None\n",
    "    \n",
    "    #Basic queries. Some of them are used as proxies to figure out the unqique queries to make later, like the \"gpu_tensor_util\" below\n",
    "    for key, query in track({\n",
    "        \"num_instances\": \"count((sum by(pod) (delta(nv_inference_request_success\"+rs+\"[\"+step+\"]))) > 0)\",\n",
    "        \"inf_rate_net\":\"sum (rate(nv_inference_count\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_rate_bypod\":\"sum by(pod) (rate(nv_inference_count\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_rate\":\"sum by(model, version, pod) (rate(nv_inference_count\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_cache_hit_rate\":\"sum by(model, version, pod) (rate(nv_cache_num_hits_per_model\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_reqs_net\":\"sum(rate(nv_inference_request_success\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_reqs_bypod\":\"sum by(pod) (rate(nv_inference_request_success\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_reqs\":\"sum by(model, version, pod) (rate(nv_inference_request_success\"+rs+\"[\"+step+\"]))\",\n",
    "        \"inf_req_dur_net\": \"avg (delta(nv_inference_request_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_que_dur_net\": \"avg (delta(nv_inference_queue_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_inp_dur_net\": \"avg (delta(nv_inference_compute_input_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_inf_dur_net\": \"avg (delta(nv_inference_compute_infer_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_out_dur_net\": \"avg (delta(nv_inference_compute_output_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_req_dur\": \"avg by(model, version, pod) (delta(nv_inference_request_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_que_dur\": \"avg by(model, version, pod) (delta(nv_inference_queue_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_inp_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_input_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_inf_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_infer_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"inf_out_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_output_duration_us\"+rs+\"[\"+step+\"])/(0.001+delta(nv_inference_request_success\"+rs+\"[\"+step+\"])))\",\n",
    "        \"gpu_tensor_util\": \"sum by(device,GPU_I_ID,instance) (avg_over_time (DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        \"gpu_dram_util\": \"sum by(device,GPU_I_ID,instance) (avg_over_time (DCGM_FI_PROF_DRAM_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        #\"inf_cache_hits\": \"avg by(model, version, pod) (delta(nv_cache_num_hits_per_model[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        }.items(), description=\"Processing Queries...\"):\n",
    "        # Build an empty list for these results; after iterating through all the timestamp pairs, they'll be concatenated together\n",
    "        results[key] = []\n",
    "        # Log the queries, as they're easier to parse after being resolved fully\n",
    "        queries.append((key, query))\n",
    "        # This function executes a query for each timestamp pair, for each key:query\n",
    "        for st, et in timestamp_tuples:\n",
    "            test_inp = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=parse_datetime(st),\n",
    "                end_time=parse_datetime(et),\n",
    "                step=step\n",
    "            )\n",
    "            # Queries are converted to a pandas dataframe\n",
    "            df = MetricRangeDataFrame(test_inp)\n",
    "            results[key].append(df)\n",
    "        # Dataframes are concatenated together along the time (index value) axis\n",
    "        results[key] = pd.concat(results[key], axis=0)\n",
    "        \n",
    "        # If we've performed a query that stores model/version info and GPU instance info, respectively, we can \n",
    "        # Create a set of unique ones for the next two sets of queries\n",
    "        if unique_model_versions is None and hasattr(results[key], \"model\") and hasattr(results[key], \"version\"):\n",
    "            unique_model_versions = set((results[key].model+\"/\"+results[key].version).values)\n",
    "        # At the EAF, the device ('nvidiaX' where X is 0...4 for example), GPU instance ID (enumeration)\n",
    "        # and the instance (IP address of host machine) are sufficient to make a unique identifier\n",
    "        if unique_gpu_instances is None and hasattr(results[key], \"GPU_I_ID\"):\n",
    "            unique_gpu_instances = set((results[key].device+\"/\"+results[key].GPU_I_ID+\"/\"+results[key].instance).values)\n",
    "    # Here we build the model-specific queries, getting both the number of unique number of Triton instances that served \n",
    "    # inference requests for this model, ad well as the inference rate of that model across all Triton instances active per time step\n",
    "    model_queries = {\"num_instances_\"+model_version: \"count((sum by(pod) (delta(nv_inference_request_success{model='\"+\n",
    "                     model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"]))) > 0)\"\n",
    "                     for model_version in unique_model_versions}\n",
    "    model_queries.update(\n",
    "        {\"inf_rate_\"+model_version: \"sum (rate(nv_inference_count{model='\"+\n",
    "         model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'\"+rsm+\"}[\"+step+\"]))\"\n",
    "         for model_version in unique_model_versions})\n",
    "    for key, query in model_queries.items():\n",
    "        queries.append((key, query))\n",
    "        results[key] = []\n",
    "        for st, et in timestamp_tuples:\n",
    "            test_inp = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=parse_datetime(st),\n",
    "                end_time=parse_datetime(et),\n",
    "                step=step\n",
    "            )\n",
    "            # The query could be empty, as a model only served in a portion of the total timerange could be inactive in some timestamp-pairs.\n",
    "            # We will deal with broadcasting these dataframes with missing values later\n",
    "            if len(test_inp) > 0:\n",
    "                df = MetricRangeDataFrame(test_inp)\n",
    "                results[key].append(df)\n",
    "        if len(results[key]) > 0:\n",
    "            results[key] = pd.concat(results[key], axis=0)\n",
    "        else:\n",
    "            # If somehow we got no results for this model query, remove it from the dictionary and avoid iterating over it later\n",
    "            try:\n",
    "                results.pop(key)\n",
    "                unique_model_versions.remove(key.replace(\"inf_rate_\", \"\").replace(\"num_instances_\", \"\"))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    # Now we gather the GPU metrics. The two most interesting ones for us are the DCGM_FI_PROF_PIPE_TENSOR_ACTIVE and \n",
    "    # DCGM_FI_PROF_DRAM_ACTIVE. The former measures how much of the compute resources (the Tensor Cores) are active, on average, in a time period\n",
    "    # If the utilization is 50%, this could mean that the tensor cores for this GPU (slice) are 100% active for 50% of the time, 50% active for\n",
    "    # 100% of the time, or any combination of activity_percent * time_active_percent that gives that product.\n",
    "    gpu_queries = {\"gpu_tensor_util_\"+str(mg): \"sum (avg_over_time(DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{\"+\n",
    "                   \"exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\"+\n",
    "                   \"device='\"+gpu_inst.split(\"/\")[0]+\"',GPU_I_ID='\"+gpu_inst.split(\"/\")[1]+\"',instance='\"+gpu_inst.split(\"/\")[2]+\"'}[\"+step+\"]))\" for mg, gpu_inst in enumerate(unique_gpu_instances)}\n",
    "    # An example of how additional labels can filter out non-matching queries, if we do \n",
    "    # DCGM_FI_PROF_DRAM_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\n",
    "    #                          device='nvidia2',GPU_I_ID='3',instance='110.4.29.45'}[120s]\n",
    "    # We'll only get metrics from that specific device, if it has a running instance with that IP, and a running GPU instance matching it\n",
    "    # In this case, for each timestep, it'll get a 'vector' of instantaenous measurements within 120s\n",
    "    # The avg_over_time function then measures the average over time of that 'vector' and produces a scalar result\n",
    "    # The scalar result may not be unique for a given timestamp, there can be other labels attached, and a final avg is taken over all\n",
    "    # of those\n",
    "    gpu_queries.update(\n",
    "        {\"gpu_dram_util_\"+str(mg): \"avg (avg_over_time(DCGM_FI_PROF_DRAM_ACTIVE{\"+\n",
    "         \"exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\"+\n",
    "        \"device='\"+gpu_inst.split(\"/\")[0]+\"',GPU_I_ID='\"+gpu_inst.split(\"/\")[1]+\"',instance='\"+gpu_inst.split(\"/\")[2]+\"'}[\"+step+\"]))\"\n",
    "         for mg, gpu_inst in enumerate(unique_gpu_instances)})\n",
    "    for key, query in gpu_queries.items():\n",
    "        queries.append((key, query))\n",
    "        results[key] = []\n",
    "        for st, et in timestamp_tuples:\n",
    "            test_inp = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=parse_datetime(st),\n",
    "                end_time=parse_datetime(et),\n",
    "                step=step\n",
    "            )\n",
    "            if len(test_inp) > 0:\n",
    "                df = MetricRangeDataFrame(test_inp)\n",
    "                results[key].append(df)\n",
    "        if len(results[key]) > 0:\n",
    "            results[key] = pd.concat(results[key], axis=0)\n",
    "            #print(key)\n",
    "        else:\n",
    "            #print(f\"results empty for {key}\")\n",
    "            try:\n",
    "                results.pop(key)\n",
    "                unique_gpu_instances.remove(key.split(\"_util_\")[1])\n",
    "            except:\n",
    "                pass\n",
    "    return results, queries, unique_model_versions, unique_gpu_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369b7e2-d0a7-426a-9023-7f5dbb33d0e8",
   "metadata": {},
   "source": [
    "In addition to the query dataframes, a list of the key:query pairs and the different model/versions active, and GPU MIG slices active, will be recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428998de-695a-4e4b-97d7-59ec468afbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results, queries, unique_model_versions, unique_gpu_instances = get_all_queries(\n",
    "#    [#(\"2023-01-30 at 00:00:00 MDT\", \"2023-02-02 at 00:00:00 MDT\"),\n",
    "     #(\"2023-02-02 at 00:00:00 MDT\", \"2023-02-05 at 00:00:00 MDT\"),\n",
    "     #(\"2023-02-05 at 00:00:00 MDT\", \"2023-02-08 at 00:00:00 MDT\"),\n",
    "     #(\"2023-02-08 at 00:00:00 MDT\", \"2023-02-11 at 00:00:00 MDT\"),\n",
    "     #(\"2023-02-11 at 00:00:00 MDT\", \"2023-02-14 at 00:00:00 MDT\"),\n",
    "     #(\"2023-02-14 at 00:00:00 MDT\", \"2023-02-17 at 00:00:00 MDT\"),\n",
    "#     (\"2023-02-17 at 00:00:00 MDT\", \"2023-02-20 at 00:00:00 MDT\"),\n",
    "#     (\"2023-02-20 at 00:00:00 MDT\", \"2023-02-23 at 00:00:00 MDT\"),\n",
    "#     (\"2023-02-23 at 00:00:00 MDT\", \"2023-02-26 at 00:00:00 MDT\"),\n",
    "#     (\"2023-02-26 at 00:00:00 MDT\", \"2023-03-01 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-01 at 00:00:00 MDT\", \"2023-03-04 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-04 at 00:00:00 MDT\", \"2023-03-07 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-07 at 00:00:00 MDT\", \"2023-03-10 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-10 at 00:00:00 MDT\", \"2023-03-13 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-13 at 00:00:00 MDT\", \"2023-03-16 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-16 at 00:00:00 MDT\", \"2023-03-19 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-19 at 00:00:00 MDT\", \"2023-03-22 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-22 at 00:00:00 MDT\", \"2023-03-25 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-25 at 00:00:00 MDT\", \"2023-03-28 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-28 at 00:00:00 MDT\", \"2023-03-31 at 00:00:00 MDT\"),\n",
    "#     (\"2023-03-31 at 00:00:00 MDT\", \"2023-04-03 at 00:00:00 MDT\"),\n",
    "#     (\"2023-04-03 at 00:00:00 MDT\", \"2023-04-05 at 12:30:00 MDT\"),\n",
    "#    ],\n",
    "#    namespace=None,\n",
    "#    step=\"60s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a06e3b-9b08-4057-a44e-0b4ddffd32de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, v in results_v2.items():\n",
    "    print(k, len(v), len(set(v.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bbd25-2e29-45d9-b043-56517d5d0c4d",
   "metadata": {},
   "source": [
    "## Converting to a unified dataframe\n",
    "This function takes the subset of results that can be concatenated into new columns,\n",
    "including the inference rate (with breakdownds by model), the timing of the inference request (overall request time, as well as broken down into queue time, input time, compute time, and output time), and the GPU dram and tensor utilization. See NVidia docs for more information on how these quantities are calculated and stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3ba7b-4f28-431a-bd0b-b70fdabf6b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_v2['inf_rate_net'][-15:], results_v2['num_instances'][-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbcb5af-c47d-453f-bf38-294581db47b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "results = copy.deepcopy(results_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a54889-8b9c-4867-ab24-fb48fa8019f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uniform_index = None\n",
    "for k, v in results.items():\n",
    "    if uniform_index is None:\n",
    "        uniform_index = set(v.index.values)\n",
    "    else:\n",
    "        uniform_index.update(set(v.index.values))\n",
    "uniform_index = list(uniform_index)\n",
    "print(len(uniform_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61bf4ae-df73-4e15-8b9b-45ba705c61c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(uniform_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee23d51-6d30-4960-be0c-6181ded46537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = results['gpu_dram_util_10'].reindex(uniform_index, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ab6bb-5ee6-4619-ba85-f418d090d823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.iloc[[results['gpu_dram_util_10'].index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2253e44-0b51-4a13-9f36-25553048abec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb8b30-3594-4bb2-ac7c-998a0ee05323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "new_index = pd.period_range(min_date, max_date)\n",
    "for key in results.keys():\n",
    "    print(key)\n",
    "    old_len = len(results[key])\n",
    "    #print(len(new_index), old_len)\n",
    "    results[key].reindex(new_index, fill_value=0)\n",
    "    new_len = len(results[key])\n",
    "    print(old_len, new_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17c422-4ef8-4711-8c12-31bc7125fd6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c53f68-c4c9-4878-bb76-4fff99f284ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xxx = results[key]\n",
    "xxx.reindex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3b472-357e-473f-a77f-d5e5e8233c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(set(results[\"inf_rate_net\"].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12b5d5-22f2-4f06-99ae-c9262b11013d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_results_to_df(results, unique_model_versions=None, unique_gpu_instances=None, add_model_stats=True, add_gpu_stats=False):\n",
    "    # This iteratively walks through some of the dataframes that are compatible and aggregates results into a \n",
    "    # unified dataframe. In each dataframe, the join call, in combination with how='left', means that results are broadcast\n",
    "    # and filled with NaN wherever results may be missing from the second of the two dataframes.\n",
    "    # For this reason, the 'inf_rate_net' which should have a valid value for all timestamps is used as the base.\n",
    "    ##idx = pd.period_range(min(df.date), max(df.date))\n",
    "    ##...: results.reindex(idx, fill_value=0)\n",
    "    min_dates = []\n",
    "    max_dates = []\n",
    "    for k, v in results.items():\n",
    "        min_dates.append(min(v.index))\n",
    "        max_dates.append(max(v.index))\n",
    "    min_date = min(min_dates)\n",
    "    max_date = max(max_dates)\n",
    "    new_index = pd.period_range(min_date, max_date)\n",
    "    for key in [\"inf_rate_net\", \"inf_reqs_net\", \"inf_req_dur_net\", \"inf_que_dur_net\", \"inf_inp_dur_net\", \"inf_inf_dur_net\", \"inf_out_dur_net\"]:\n",
    "        results[key].reindex(new_index, fill_value=0)\n",
    "    i0 = results[\"inf_rate_net\"].join(results[\"num_instances\"],\n",
    "                                      how=\"left\", \n",
    "                                      rsuffix=\"_num_instances\",\n",
    "                                     )\n",
    "    # We use the rsuffix and lsuffix to convert column names from 'value' to one that is understandable/parseable later on. \n",
    "    i0 = i0.join(results[\"inf_reqs_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_reqs_net\")\n",
    "    i0 = i0.join(results[\"inf_req_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_req_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_que_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_que_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_inp_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_inp_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_inf_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_inf_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_out_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_out_dur_net\")\n",
    "    \n",
    "    #Add the model metrics, using some suffix parsing to make it into num_instances_X or rate_X where X is the model name\n",
    "    if add_model_stats:\n",
    "        for model in track(unique_model_versions, description=\"Adding Model Stats\"):\n",
    "            itemp = results[\"inf_rate_\" + model].join(results[\"num_instances_\" + model],\n",
    "                                                      how=\"left\",\n",
    "                                                      rsuffix=\"_num_instances_\"+model.split(\"/\")[0],\n",
    "                                                      lsuffix=\"_rate_\"+model.split(\"/\")[0],\n",
    "                                                     )\n",
    "            i0 = i0.join(itemp, how=\"left\")\n",
    "        \n",
    "    #Add the GPU Instance metrics, including GPU instance enumeration\n",
    "    if add_gpu_stats:\n",
    "        for mg, gpu in track(enumerate(unique_gpu_instances), description=\"Adding GPU Stats\"):\n",
    "            results[\"gpu_tensor_util_\" + str(mg)].fillna(0, inplace=True)\n",
    "            results[\"gpu_dram_util_\" + str(mg)].fillna(0, inplace=True)\n",
    "            itemp = results[\"gpu_tensor_util_\" + str(mg)].join(results[\"gpu_dram_util_\" + str(mg)],\n",
    "                                                      how=\"left\",\n",
    "                                                      rsuffix=\"_gpu_dram_util_\"+str(mg),\n",
    "                                                      lsuffix=\"_gpu_tensor_util_\"+str(mg),\n",
    "                                                     )\n",
    "            i0 = i0.join(itemp, how=\"left\")\n",
    "\n",
    "    #Get rid of the \"value\" in column names, and fill NaN values with 0 everywhere\n",
    "    i0.rename(columns={\"value\": \"rate\"}, inplace=True)\n",
    "    i0.rename(columns={col:col[6:] for col in i0.columns if col.startswith(\"value_\")}, inplace=True)\n",
    "    i0.fillna(0, inplace=True)\n",
    "    \n",
    "    # Aggregate some stats for models\n",
    "    # The summed rate and total inference rate should match, otherwise we've double-counted something\n",
    "    # The summed instances may NOT match: if a model is active on 5 of 10 servers in a timestep, and another is active on 7 of 10\n",
    "    # Then there will be '12' active instances in that timestep, net. This number divided by the net_instances\n",
    "    # Therefore gives a measure of the 'average' model concurrency in a timestep. 10 net_instances and 70 summed_intstances\n",
    "    # would indicate each instances was serving 7 models at some point in that timestep (but this is a lossy gathering of information,\n",
    "    # 6 models could do one inference request while the last model is responsible for all of the remainder of thousands of requests.\n",
    "    valid_model_keys = [col for col in i0.columns if col.startswith(\"rate_\") and col.replace(\"rate_\", \"num_instances_\") in i0.columns]\n",
    "    i0[\"summed_rate\"] = sum([i0[col] for col in valid_model_keys])\n",
    "    i0[\"summed_instances\"] = sum([i0[col.replace(\"rate_\", \"num_instances_\")] for col in valid_model_keys])\n",
    "    \n",
    "    # Aggregate some stats for GPU instances\n",
    "    valid_gpu_keys = [col for col in i0.columns if col.startswith(\"gpu_tensor_util\") and col.replace(\"tensor\", \"dram\") in i0.columns]\n",
    "    i0[\"summed_gpu_tensor_util\"] = sum([i0[col] for col in valid_gpu_keys])\n",
    "    i0[\"summed_gpu_dram_util\"] = sum([i0[col.replace(\"tensor\", \"dram\")] for col in valid_model_keys])\n",
    "    return i0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a8b3d-7d38-4b98-8a3a-2d283f4990e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i0 = convert_results_to_df(results_v2, unique_model_versions_v2, unique_gpu_instances_v2, False, True)\n",
    "i0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad9563-4afe-4c91-ba16-7eb6ab66fb14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfad28-d91a-4dec-94e1-a2c317378d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.rint(i0.num_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fb821-630e-45bd-8fed-7d5fc1cfd067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \n",
    "      \"tab:brown\", \"tab:gray\", \"tab:olive\", \"tab:cyan\", \"tab:pink\"]\n",
    "for nm, mod in enumerate(unique_model_versions):\n",
    "    model = mod.split(\"/\")[0]\n",
    "    ii = i0[getattr(i0, \"rate_\"+model) > 0]\n",
    "    plt.scatter(ii.inf_reqs_net, getattr(ii, \"rate_\"+model), c=cc[nm], label=model, alpha=0.2)\n",
    "plt.scatter(i0.inf_reqs_net, i0.rate, c=cc[-1], label=\"All\", alpha=0.2)\n",
    "plt.ylabel(\"Inference Rate\")\n",
    "plt.xlabel(\"Inference Requests [All]\")\n",
    "plt.legend()\n",
    "plt.title(\"Rate vs Inference Requests\")\n",
    "plt.savefig(\"rate_vs_requests.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7744582f-3227-4ce2-b33b-e1d06f5b9e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \n",
    "      \"tab:brown\", \"tab:gray\", \"tab:olive\", \"tab:cyan\", \"tab:pink\"]\n",
    "for nm, mod in enumerate(unique_model_versions):\n",
    "    model = mod.split(\"/\")[0]\n",
    "    ii = i0[getattr(i0, \"rate_\"+model) > 0]\n",
    "    plt.scatter(getattr(ii, \"num_instances_\"+model), getattr(ii, \"rate_\"+model), c=cc[nm], label=model, alpha=0.2)\n",
    "plt.scatter(i0.num_instances, i0.rate, c=cc[-1], label=\"All\", alpha=0.2)\n",
    "plt.ylabel(\"Inference Rate\")\n",
    "plt.xlabel(\"Active Triton Servers\")\n",
    "plt.legend()\n",
    "plt.savefig(\"rate_vs_servers_scatter.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f720cc-13e4-45bc-a5cd-26e96c369476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \n",
    "      \"tab:brown\", \"tab:gray\", \"tab:olive\", \"tab:cyan\", \"tab:pink\"]\n",
    "iter_models = unique_model_versions.union({\"All/1\"})\n",
    "print({mod:mod for mod in iter_models})\n",
    "data   = {mod.split(\"/\")[0]: [] for mod in iter_models}\n",
    "points = {mod.split(\"/\")[0]: [] for mod in iter_models}\n",
    "colors = {mod.split(\"/\")[0]: [] for mod in iter_models}\n",
    "print(data.keys())\n",
    "labels = []\n",
    "for act_srv in range(1, 11):\n",
    "    for nm, mod in enumerate(unique_model_versions):\n",
    "        model = mod.split(\"/\")[0]\n",
    "        if model == \"\":\n",
    "            continue\n",
    "        ii=None\n",
    "        ii = i0[(getattr(i0, \"rate_\"+model) > 0) & (getattr(i0, \"num_instances_\"+model) == act_srv)]\n",
    "        if len(ii) == 0:\n",
    "            #print(\"Skipping\")\n",
    "            continue\n",
    "        data[model].append(getattr(ii, \"rate_\"+model))\n",
    "        colors[model].append(cc[nm])\n",
    "        points[model].append(act_srv)\n",
    "    model = \"All\"\n",
    "    ii = i0[(getattr(i0, \"rate\") > 0) & (getattr(i0, \"num_instances\") == act_srv)]\n",
    "    if len(ii) == 0:\n",
    "        continue\n",
    "    data[model].append(getattr(ii, \"rate\"))\n",
    "    colors[model].append(cc[-1])\n",
    "    points[model].append(act_srv)\n",
    "\n",
    "for mod in unique_model_versions:\n",
    "    model = mod.split(\"/\")[0]\n",
    "    plt.violinplot(data[model], points[model])\n",
    "#plt.scatter(i0.num_instances, i0.rate, c=cc[-1], label=\"All\")\n",
    "plt.ylabel(\"Inference Rate\")\n",
    "plt.xlabel(\"Active Triton Servers\")\n",
    "plt.legend()\n",
    "plt.savefig(\"rate_vs_servers_violin_models.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba46a9-9f65-431a-b2fc-40de18e4c570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for mod in {\"All/1\"}:\n",
    "    model = mod.split(\"/\")[0]\n",
    "    print(model)\n",
    "    #print([len(xx) for xx in data[model]])\n",
    "    print(points[model])\n",
    "    plt.violinplot(data[model], points[model])\n",
    "#plt.scatter(i0.num_instances, i0.rate, c=cc[-1], label=\"All\")\n",
    "plt.ylabel(\"Inference Rate\")\n",
    "plt.xlabel(\"Active Triton Servers\")\n",
    "plt.legend()\n",
    "plt.savefig(\"rate_vs_servers_violin_net.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb562da6-d658-449d-a1a6-c05db9a40f17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isort = i0.sort_values(\"num_instances\", axis=0, ascending=True)\n",
    "cc = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \n",
    "      \"tab:brown\", \"tab:gray\", \"tab:olive\", \"tab:cyan\", \"tab:pink\"]\n",
    "val = {\"inf_que_dur_net\": [],\n",
    "       \"inf_inp_dur_net\": [],\n",
    "       \"inf_inf_dur_net\": [],\n",
    "       \"inf_out_dur_net\": [],\n",
    "       \"inf_req_dur_net\": [],\n",
    "       }\n",
    "err = {\"inf_que_dur_net\": [],\n",
    "       \"inf_inp_dur_net\": [],\n",
    "       \"inf_inf_dur_net\": [],\n",
    "       \"inf_out_dur_net\": [],\n",
    "       \"inf_req_dur_net\": [],\n",
    "       }\n",
    "ninst = [x for x in range(int(np.max(isort.num_instances)))]\n",
    "for ats in ninst:\n",
    "    icut = isort[isort.num_instances == ats]\n",
    "    val[\"inf_que_dur_net\"].append(np.mean(icut.inf_que_dur_net/icut.inf_req_dur_net))\n",
    "    err[\"inf_que_dur_net\"].append(np.sqrt(np.var(icut.inf_que_dur_net/icut.inf_req_dur_net)))\n",
    "    val[\"inf_inp_dur_net\"].append(np.mean(icut.inf_inp_dur_net/icut.inf_req_dur_net))\n",
    "    err[\"inf_inp_dur_net\"].append(np.sqrt(np.var(icut.inf_inp_dur_net/icut.inf_req_dur_net)))\n",
    "    val[\"inf_inf_dur_net\"].append(np.mean(icut.inf_inf_dur_net/icut.inf_req_dur_net))\n",
    "    err[\"inf_inf_dur_net\"].append(np.sqrt(np.var(icut.inf_inf_dur_net/icut.inf_req_dur_net)))\n",
    "    val[\"inf_out_dur_net\"].append(np.mean(icut.inf_out_dur_net/icut.inf_req_dur_net))\n",
    "    err[\"inf_out_dur_net\"].append(np.sqrt(np.var(icut.inf_out_dur_net/icut.inf_req_dur_net)))\n",
    "    val[\"inf_req_dur_net\"].append(np.mean(icut.inf_req_dur_net))\n",
    "    err[\"inf_req_dur_net\"].append(np.sqrt(np.var(icut.inf_req_dur_net)))\n",
    "for k in val:\n",
    "    val[k] = np.array(val[k])\n",
    "for k in err:\n",
    "    err[k] = np.array(err[k])\n",
    "    \n",
    "#for nm, mod in enumerate(unique_model_versions):\n",
    "#    model = mod.split(\"/\")[0]\n",
    "#    ii = isort[getattr(isort, \"rate_\"+model) > 0]\n",
    "#    plt.plot(getattr(isort, \"num_instances_\"+model), getattr(isort, \"rate_\"+model), c=cc[nm], label=model)\n",
    "#plt.plot(isort.num_instances, isort.inf_req_dur_net, c=cc[0], label=\"Req. Dur.\")\n",
    "plt.plot(ninst, val[\"inf_que_dur_net\"], c=cc[1], label=\"Que. Dur.\")\n",
    "plt.fill_between(ninst, val[\"inf_que_dur_net\"]-err[\"inf_que_dur_net\"], val[\"inf_que_dur_net\"]+err[\"inf_que_dur_net\"], color=cc[1], alpha=0.3)\n",
    "plt.plot(ninst, val[\"inf_inp_dur_net\"], c=cc[2], label=\"Inp. Dur.\")\n",
    "plt.fill_between(ninst, val[\"inf_inp_dur_net\"]-err[\"inf_inp_dur_net\"], val[\"inf_inp_dur_net\"]+err[\"inf_inp_dur_net\"], color=cc[2], alpha=0.3)\n",
    "plt.plot(ninst, val[\"inf_inf_dur_net\"], c=cc[3], label=\"Inf. Dur.\")\n",
    "plt.fill_between(ninst, val[\"inf_inf_dur_net\"]-err[\"inf_inf_dur_net\"], val[\"inf_inf_dur_net\"]+err[\"inf_inf_dur_net\"], color=cc[3], alpha=0.3)\n",
    "plt.plot(ninst, val[\"inf_out_dur_net\"], c=cc[4], label=\"Out. Dur.\")\n",
    "plt.fill_between(ninst, val[\"inf_out_dur_net\"]-err[\"inf_out_dur_net\"], val[\"inf_out_dur_net\"]+err[\"inf_out_dur_net\"], color=cc[4], alpha=0.3)\n",
    "plt.ylabel(\"Fraction of Request Duration\")\n",
    "plt.xlabel(\"Active Triton Servers\")\n",
    "plt.legend()\n",
    "plt.savefig(\"duration_ratio_vs_servers.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9b1224-5263-4508-99d4-c6ed381fd2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(ninst, val[\"inf_req_dur_net\"], c=cc[5], label=\"Req. Dur.\")\n",
    "plt.fill_between(ninst, val[\"inf_req_dur_net\"]-err[\"inf_req_dur_net\"], val[\"inf_req_dur_net\"]+err[\"inf_req_dur_net\"], color=cc[5], alpha=0.3)\n",
    "plt.ylabel(\"Request Duration [$\\mu s$]\")\n",
    "plt.xlabel(\"Active Triton Servers\")\n",
    "plt.legend()\n",
    "plt.savefig(\"request_duration_vs_servers.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c9e82-82df-48f3-82db-db9e1be6a5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i0[(getattr(i0, \"rate\") > 0) & (getattr(i0, \"num_instances\") == act_srv)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ace97a-8520-40d0-a457-a9ec5d0770fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request duration as function of... servers, rate, both?\n",
    "cc = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \n",
    "      \"tab:brown\", \"tab:gray\", \"tab:olive\", \"tab:cyan\", \"tab:pink\"]\n",
    "iter_models = unique_model_versions.union({\"All/1\"})\n",
    "print({mod:mod for mod in iter_models})\n",
    "data   = {mod.split(\"/\")[0]: [] for mod in iter_models}\n",
    "points = {mod.split(\"/\")[0]: [] for mod in iter_models}\n",
    "colors = {mod.split(\"/\")[0]: [] for mod in iter_models}\n",
    "print(data.keys())\n",
    "labels = []\n",
    "for act_srv in range(1, 11):\n",
    "    ii = i0[(getattr(i0, \"rate\") > 0) & (getattr(i0, \"num_instances\") == act_srv)]\n",
    "    for nm, dur_type in enumerate([\"req\", \"que\", \"inp\", \"inf\", \"out\"]):\n",
    "        if len(ii) == 0:\n",
    "            #print(\"Skipping\")\n",
    "            continue\n",
    "        data[model].append(getattr(ii, \"rate_\"+model))\n",
    "        colors[model].append(cc[nm])\n",
    "        points[model].append(act_srv)\n",
    "    model = \"All\"\n",
    "    ii = i0[(getattr(i0, \"rate\") > 0) & (getattr(i0, \"num_instances\") == act_srv)]\n",
    "    if len(ii) == 0:\n",
    "        continue\n",
    "    data[model].append(getattr(ii, \"rate\"))\n",
    "    colors[model].append(cc[-1])\n",
    "    points[model].append(act_srv)\n",
    "\n",
    "for mod in unique_model_versions:\n",
    "    model = mod.split(\"/\")[0]\n",
    "    plt.violinplot(data[model], points[model])\n",
    "#plt.scatter(i0.num_instances, i0.rate, c=cc[-1], label=\"All\")\n",
    "plt.ylabel(\"Inference Rate\")\n",
    "plt.xlabel(\"Active Triton Servers\")\n",
    "plt.legend()\n",
    "plt.savefig(\"rate_vs_servers_violin_models.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f8a0c-7c52-425d-b425-64e51cf13332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Violin plot of inference server frequency -> two modes?\n",
    "#violin_parts_1 = plt.violinplot(i0.num_instances[i0.num_instances < 2])\n",
    "#for pc in violin_parts_1['bodies']:\n",
    "#    pc.set_facecolor('red')\n",
    "#    pc.set_edgecolor('blue')\n",
    "violin_parts_2 = plt.violinplot(i0.num_instances[i0.num_instances > 1])\n",
    "for pc in violin_parts_2['bodies']:\n",
    "    pc.set_facecolor('green')\n",
    "    pc.set_edgecolor('blue')\n",
    "plt.ylabel(\"Number of Active Triton Servers\")\n",
    "plt.xlabel(\"Relative Frequency\")\n",
    "plt.title(\"Active Triton Server Distribution\")\n",
    "plt.savefig(\"active_servers_except_one.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245bb10f-d1c0-49ac-b700-feffbb5dba36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Histogram a selection of distributions\n",
    "from IPython.display import display, clear_output\n",
    "hists = {}\n",
    "for col in [\"rate\", \"num_instances\", \"inf_reqs_net\", \n",
    "            \"inf_req_dur_net\", \"inf_que_dur_net\", \"inf_inp_dur_net\", \"inf_inf_dur_net\", \"inf_out_dur_net\",\n",
    "            \"summed_rate\", \"summed_instances\", \"summed_gpu_tensor_util\", \"summed_gpu_dram_util\",]:\n",
    "    print(col)\n",
    "    ii = i0[i0.rate > 0]\n",
    "    x1, x2 = np.min(ii[col].values), np.max(ii[col].values)\n",
    "    figure, ax = plt.subplots(figsize=(4,5))\n",
    "    hists[col] = hist.Hist(hist.axis.Regular(10, x1, x2, name=col, label=col),\n",
    "                           hist.storage.Double())\n",
    "    hists[col].fill(ii[col].values)\n",
    "    hists[col].plot(ax=ax)\n",
    "    ax.set_ylabel(\"Frequency [120s subsampling]\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    #display(figure)\n",
    "    figure.savefig(col+\"_hist.pdf\")\n",
    "    #clear_output(wait=True); plt.pause(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcf2c6-365d-4d75-a815-34433744c4e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc = [\"tab:blue\", \"tab:green\", \"tab:red\", \"tab:orange\", \"tab:purple\", \"tab:gray\", \"tab:pink\"]\n",
    "ninst = [x for x in range(1, int(np.max(isort.num_instances)))]\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=len(ninst),\n",
    "    sharex=False,\n",
    "    sharey=True,\n",
    "    squeeze=True,\n",
    "    #width_ratios=[1 for x in ninst],\n",
    "    height_ratios=None,\n",
    "    #subplot_kw=None,\n",
    "    gridspec_kw={\"width_ratios\":[1 for x in ninst],\n",
    "                 \"wspace\": 0,\n",
    "                },\n",
    "    figsize=(30, 15)\n",
    "    #**fig_kw,\n",
    ")\n",
    "for nax, ats in enumerate(ninst):\n",
    "    icut = isort[np.isclose(isort.num_instances,ats)]\n",
    "    ax[nax].scatter(icut.inf_reqs_net, icut.rate/ats, color = \"tab:blue\", alpha=0.2, label=\"All\")\n",
    "    #imodel = isort[np.isclose(\n",
    "    if nax == 0:\n",
    "        ax[nax].set_ylabel(r\"$\\frac{<Inference Rate>}{Triton Server}$\")\n",
    "        ax[nax].set_xlim(0, 50)\n",
    "    if ats == ninst[-1]:\n",
    "        ax[nax].set_xlabel(r\"$\\frac{Inf. Req.}{Triton Server}$\")\n",
    "    ax[nax].legend()\n",
    "fig.savefig(\"rates_vs_avgreqsnet_byinstances.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e2c81-8337-4d98-b329-bdddd477645a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc = [\"tab:blue\", \"tab:green\", \"tab:red\", \"tab:orange\", \"tab:purple\", \"tab:gray\", \"tab:pink\"]\n",
    "ninst = [x for x in range(1, int(np.max(isort.num_instances)))]\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=len(ninst),\n",
    "    sharex=False,\n",
    "    sharey=True,\n",
    "    squeeze=True,\n",
    "    #width_ratios=[1 for x in ninst],\n",
    "    height_ratios=None,\n",
    "    #subplot_kw=None,\n",
    "    gridspec_kw={\"width_ratios\":[1 for x in ninst],\n",
    "                 \"wspace\": 0,\n",
    "                },\n",
    "    figsize=(24, 8)\n",
    "    #**fig_kw,\n",
    ")\n",
    "for nax, ats in enumerate(ninst):\n",
    "    icut = isort[np.isclose(isort.num_instances,ats)]\n",
    "    ax[nax].scatter(icut.inf_reqs_net/ats, icut.inf_req_dur_net, color = \"tab:blue\", alpha=0.2, label=\"Req.\")\n",
    "    ax[nax].scatter(icut.inf_reqs_net/ats, icut.inf_que_dur_net, color = \"tab:red\", alpha=0.05, label=\"Que.\")\n",
    "    ax[nax].scatter(icut.inf_reqs_net/ats, icut.inf_inf_dur_net, color = \"tab:green\", alpha=0.2, label=\"Inf.\")\n",
    "    #imodel = isort[np.isclose(\n",
    "    if nax == 0:\n",
    "        ax[nax].set_ylabel(r\"$\\frac{<Inf. Req. Dur.>}{Triton Server}$\")\n",
    "        ax[nax].set_ylim(0, 0.3*ax[nax].get_ylim()[1])\n",
    "        ax[nax].set_xlim(0, 50)\n",
    "    if ats == ninst[-1]:\n",
    "        ax[nax].set_xlabel(r\"$\\frac{Inf. Req.}{Triton Server}$\")\n",
    "        ax[nax].legend()\n",
    "fig.savefig(\"durations_vs_avgreqsnet_byinstances.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8dc76-9e88-4aa9-a148-705bbff1126a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(i0.summed_gpu_tensor_util, i0.rate) #, 'summed_gpu_dram_util'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff877e-255d-42b4-a847-23f6df817ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.plot(i0.index, i0.summed_gpu_tensor_util)\n",
    "#plt.plot(i0.index, i0.rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665aed7-14e7-4fa3-a2b6-b0c5eedd920f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(i0.summed_gpu_dram_util, i0.rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47751fd-5d68-4942-b575-1b23e16f2473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hist\n",
    "h = hist.Hist(hist.axis.Regular(24, 0, 120000, name=\"rate\"),\n",
    "              hist.axis.Integer(1, 12, name=\"instances\"),\n",
    "              hist.storage.Double()\n",
    "             )\n",
    "h.fill(rate=i0.rate, instances=i0.num_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ec500-9e86-4d5c-9e4b-b8db8e0c7086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852301f-ce66-417c-9917-c87672dd26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "'inf_reqs_net', 'inf_req_dur_net',\n",
    "       'inf_que_dur_net', 'inf_inp_dur_net', 'inf_inf_dur_net',\n",
    "       'inf_out_dur_net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e02198-b119-4df0-a73d-66e8770f5860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save results in a pickle file for later\n",
    "import pickle\n",
    "with open(f\"triton_metrics_test.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(i0, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8f557-ff61-47c7-b643-4740e11d6779",
   "metadata": {},
   "source": [
    "## A few simple plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243ca52-500b-408c-91c3-ab5393c1b1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(i0.index.values, i0.rate.values)\n",
    "scale_value = max(i0.rate.values)/max(i0.summed_gpu_tensor_util)\n",
    "plt.plot(i0.index.values, i0.summed_gpu_tensor_util.values*scale_value, color=\"tab:red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6c219-49ca-49d6-ab2a-e9c00707dcd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the rate versus number of instances, where at least 1 active instance is serving results\n",
    "plt.scatter(\"num_instances\", \"rate\", data=i0[i0.num_instances > 0], color=\"tab:red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58424494-1247-4eee-9b46-7867e6ef21b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the same thing, but specific to the pn_demo model\n",
    "plt.scatter(\"num_instances_pn_demo\", \"rate_pn_demo\", data=i0[i0.num_instances > 0], color=\"tab:blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ddc95-759c-4451-ae45-ab313e4b42c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(\"num_instances_svj_tch_gnn\", \"rate_svj_tch_gnn\", data=i0[i0.num_instances > 0], color=\"tab:green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39053bf5-269d-4994-bf75-47aabc2b055b",
   "metadata": {},
   "source": [
    "## Concurrency\n",
    "How can we measure how many models are active per Triton server? The ```num_instances``` is how many actives servers there are.\n",
    "The variables ```summed_instances``` is the sum of each model's active ```num_instances```. If the values are equal, then concurrency is low\n",
    "(when defined as the number of ML models being run on an individual server). If ```summed_instances >> num_instances```, that indicates that each triton server is tending to actively serve requests from multiple models in a given timespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b912b07-fbd3-431b-8d1c-330b63f73aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Concurrency question: if models tend to gravitate to their own instances, summed instances ~ num_instances\n",
    "#If concurrency is as high as possible, summed instances ~ avg_num_models * num_instances\n",
    "ii = i0[i0.num_instances > 0].summed_instances/i0[i0.num_instances > 0].num_instances\n",
    "print(np.mean(ii), np.max(ii), np.min(ii))\n",
    "print(np.sqrt(np.var(ii)))\n",
    "\n",
    "#Consistency check: summed rate should always add to net rate!\n",
    "kk = i0[i0.num_instances > 0].summed_rate/i0[i0.num_instances > 0].rate\n",
    "print(np.mean(kk), np.max(kk), np.min(kk))\n",
    "print(np.sqrt(np.var(kk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e125dc-31af-4d94-828b-a588d07877de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea for triton",
   "language": "python",
   "name": "coffea-triton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
