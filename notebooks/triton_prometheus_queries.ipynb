{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2c7aa-75a0-45aa-9cc2-e7214dbb41ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import json\n",
    "import prometheus_api_client\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from prometheus_api_client.metric_range_df import MetricRangeDataFrame\n",
    "from prometheus_api_client.metric_snapshot_df import MetricSnapshotDataFrame\n",
    "from prometheus_api_client.metrics_list import MetricsList\n",
    "from prometheus_api_client.utils import parse_datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8010b-3b64-4c9c-9733-933144858a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prom = PrometheusConnect(url=\"http://lsdataitb.fnal.gov:9009/prometheus\", disable_ssl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c85d9-0dc5-42f0-b43d-0476e5836864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xxx = prom.all_metrics()\n",
    "len(xxx)\n",
    "[xx for xx in xxx if \"GPU\" in xx or \"DCGM\" in xx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f2046-6e41-4340-b8cc-94ebfd43a5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_queries(timestamp_tuples, step):\n",
    "    results = {}\n",
    "    queries = []\n",
    "    timestamp_tuples = [(\"2023-03-30 at 16:00:00 MDT\", \"2023-03-30 at 19:00:00 MDT\"),\n",
    "                       ]\n",
    "    unique_model_versions = None\n",
    "    unique_gpu_instances = None\n",
    "    for key, query in {\n",
    "        \"num_instances\": \"count((sum by(pod) (delta(nv_inference_request_success[\"+step+\"]))) > 0)\",\n",
    "        \"inf_rate_net\":\"sum (rate(nv_inference_count[\"+step+\"]))\",\n",
    "        \"inf_rate_bypod\":\"sum by(pod) (rate(nv_inference_count[\"+step+\"]))\",\n",
    "        \"inf_rate\":\"sum by(model, version, pod) (rate(nv_inference_count[\"+step+\"]))\",\n",
    "        \"inf_cache_hit_rate\":\"sum by(model, version, pod) (rate(nv_cache_num_hits_per_model[\"+step+\"]))\",\n",
    "        \"inf_reqs_net\":\"sum(rate(nv_inference_request_success[\"+step+\"]))\",\n",
    "        \"inf_reqs_bypod\":\"sum by(pod) (rate(nv_inference_request_success[\"+step+\"]))\",\n",
    "        \"inf_reqs\":\"sum by(model, version, pod) (rate(nv_inference_request_success[\"+step+\"]))\",\n",
    "        \"inf_req_dur_net\": \"avg (delta(nv_inference_request_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_que_dur_net\": \"avg (delta(nv_inference_queue_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_inp_dur_net\": \"avg (delta(nv_inference_compute_input_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_inf_dur_net\": \"avg (delta(nv_inference_compute_infer_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_out_dur_net\": \"avg (delta(nv_inference_compute_output_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_req_dur\": \"avg by(model, version, pod) (delta(nv_inference_request_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_que_dur\": \"avg by(model, version, pod) (delta(nv_inference_queue_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_inp_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_input_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_inf_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_infer_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"inf_out_dur\": \"avg by(model, version, pod) (delta(nv_inference_compute_output_duration_us[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        \"gpu_tensor_util\": \"sum by(device,GPU_I_ID,instance) (avg_over_time (DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        \"gpu_dram_util\": \"sum by(device,GPU_I_ID,instance) (avg_over_time (DCGM_FI_PROF_DRAM_ACTIVE{exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0'}[\"+step+\"]))\",\n",
    "        #\"inf_cache_hits\": \"avg by(model, version, pod) (delta(nv_cache_num_hits_per_model[\"+step+\"])/(1+1000000*delta(nv_inference_request_success[\"+step+\"])))\",\n",
    "        }.items():\n",
    "        results[key] = []\n",
    "        queries.append((key, query))\n",
    "        for st, et in timestamp_tuples:\n",
    "            test_inp = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=parse_datetime(st),\n",
    "                end_time=parse_datetime(et),\n",
    "                step=step\n",
    "            )\n",
    "            df = MetricRangeDataFrame(test_inp)\n",
    "            results[key].append(df)\n",
    "        results[key] = pd.concat(results[key], axis=0)\n",
    "        if unique_model_versions is None and hasattr(results[key], \"model\") and hasattr(results[key], \"version\"):\n",
    "            unique_model_versions = set((results[key].model+\"/\"+results[key].version).values)\n",
    "        if unique_gpu_instances is None and hasattr(results[key], \"GPU_I_ID\"):\n",
    "            unique_gpu_instances = set((results[key].device+\"/\"+results[key].GPU_I_ID+\"/\"+results[key].instance).values)\n",
    "    model_queries = {\"num_instances_\"+model_version: \"count((sum by(pod) (delta(nv_inference_request_success{model='\"+\n",
    "                     model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'}[\"+step+\"]))) > 0)\"\n",
    "                     for model_version in unique_model_versions}\n",
    "    model_queries.update(\n",
    "        {\"inf_rate_\"+model_version: \"sum (rate(nv_inference_count{model='\"+\n",
    "         model_version.split(\"/\")[0]+\"',version='\"+model_version.split(\"/\")[1]+\"'}[\"+step+\"]))\"\n",
    "         for model_version in unique_model_versions})\n",
    "    for key, query in model_queries.items():\n",
    "        queries.append((key, query))\n",
    "        results[key] = []\n",
    "        for st, et in timestamp_tuples:\n",
    "            test_inp = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=parse_datetime(st),\n",
    "                end_time=parse_datetime(et),\n",
    "                step=step\n",
    "            )\n",
    "            if len(test_inp) > 0:\n",
    "                df = MetricRangeDataFrame(test_inp)\n",
    "                results[key].append(df)\n",
    "        if len(results[key]) > 0:\n",
    "            results[key] = pd.concat(results[key], axis=0)\n",
    "        else:\n",
    "            results.pop(key)\n",
    "            unique_model_versions.remove(key.split(\"_instances_\")[1])\n",
    "    gpu_queries = {\"gpu_tensor_util_\"+str(mg): \"sum (avg_over_time(DCGM_FI_PROF_PIPE_TENSOR_ACTIVE{\"+\n",
    "                   \"exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\"+\n",
    "                   \"device='\"+gpu_inst.split(\"/\")[0]+\"',GPU_I_ID='\"+gpu_inst.split(\"/\")[1]+\"',instance='\"+gpu_inst.split(\"/\")[2]+\"'}[\"+step+\"]))\" for mg, gpu_inst in enumerate(unique_gpu_instances)}\n",
    "    gpu_queries.update(\n",
    "        {\"gpu_dram_util_\"+str(mg): \"avg (avg_over_time(DCGM_FI_PROF_DRAM_ACTIVE{\"+\n",
    "         \"exported_container='triton',exported_namespace='triton',prometheus_replica='prometheus-k8s-0',\"+\n",
    "        \"device='\"+gpu_inst.split(\"/\")[0]+\"',GPU_I_ID='\"+gpu_inst.split(\"/\")[1]+\"',instance='\"+gpu_inst.split(\"/\")[2]+\"'}[\"+step+\"]))\"\n",
    "         for mg, gpu_inst in enumerate(unique_gpu_instances)})\n",
    "    for key, query in gpu_queries.items():\n",
    "        queries.append((key, query))\n",
    "        results[key] = []\n",
    "        for st, et in timestamp_tuples:\n",
    "            test_inp = prom.custom_query_range(\n",
    "                query=query,\n",
    "                start_time=parse_datetime(st),\n",
    "                end_time=parse_datetime(et),\n",
    "                step=step\n",
    "            )\n",
    "            if len(test_inp) > 0:\n",
    "                df = MetricRangeDataFrame(test_inp)\n",
    "                results[key].append(df)\n",
    "        if len(results[key]) > 0:\n",
    "            results[key] = pd.concat(results[key], axis=0)\n",
    "            #print(key)\n",
    "        else:\n",
    "            #print(f\"results empty for {key}\")\n",
    "            results.pop(key)\n",
    "            unique_gpu_instances.remove(key.split(\"_util_\")[1])\n",
    "    return results, queries, unique_model_versions, unique_gpu_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428998de-695a-4e4b-97d7-59ec468afbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results, queries, unique_model_versions, unique_gpu_instances = get_all_queries([(\"2023-03-30 at 16:00:00 MDT\", \"2023-03-30 at 19:00:00 MDT\"),], step=\"60s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb863da4-dc44-4e44-863a-962e306b3f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_model_versions, unique_gpu_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7245fa8-2eed-4b0a-8ff4-972c9579c6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[\"gpu_tensor_util_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12b5d5-22f2-4f06-99ae-c9262b11013d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_results_to_df(results):\n",
    "    i0 = results[\"inf_rate_net\"].join(results[\"num_instances\"],\n",
    "                                      how=\"left\", \n",
    "                                      rsuffix=\"_num_instances\",\n",
    "                                     )\n",
    "    i0 = i0.join(results[\"inf_reqs_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_reqs_net\")\n",
    "    i0 = i0.join(results[\"inf_req_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_req_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_que_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_que_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_inp_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_inp_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_inf_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_inf_dur_net\")\n",
    "    i0 = i0.join(results[\"inf_out_dur_net\"],\n",
    "                 how=\"left\",\n",
    "                 rsuffix=\"_inf_out_dur_net\")\n",
    "    \n",
    "    #Add the model metrics\n",
    "    for model in unique_model_versions:\n",
    "        itemp = results[\"inf_rate_\" + model].join(results[\"num_instances_\" + model],\n",
    "                                                  how=\"left\",\n",
    "                                                  rsuffix=\"_num_instances_\"+model.split(\"/\")[0],\n",
    "                                                  lsuffix=\"_rate_\"+model.split(\"/\")[0],\n",
    "                                                 )\n",
    "        i0 = i0.join(itemp, how=\"left\")\n",
    "        \n",
    "    #Add the GPU Instance metrics\n",
    "    for mg, gpu in enumerate(unique_gpu_instances):\n",
    "        results[\"gpu_tensor_util_\" + str(mg)].fillna(0, inplace=True)\n",
    "        results[\"gpu_dram_util_\" + str(mg)].fillna(0, inplace=True)\n",
    "        itemp = results[\"gpu_tensor_util_\" + str(mg)].join(results[\"gpu_dram_util_\" + str(mg)],\n",
    "                                                  how=\"left\",\n",
    "                                                  rsuffix=\"_gpu_dram_util_\"+str(mg),\n",
    "                                                  lsuffix=\"_gpu_tensor_util_\"+str(mg),\n",
    "                                                 )\n",
    "        i0 = i0.join(itemp, how=\"left\")\n",
    "\n",
    "    #Get rid of the \"value\" in names, and fill NaN values with 0 everywhere\n",
    "    i0.rename(columns={\"value\": \"rate\"}, inplace=True)\n",
    "    i0.rename(columns={col:col[6:] for col in i0.columns if col.startswith(\"value_\")}, inplace=True)\n",
    "    i0.fillna(0, inplace=True)\n",
    "    \n",
    "    #Aggregate some stats for models\n",
    "    valid_model_keys = [col for col in i0.columns if col.startswith(\"rate_\") and col.replace(\"rate_\", \"num_instances_\") in i0.columns]\n",
    "    i0[\"summed_rate\"] = sum([i0[col] for col in valid_model_keys])\n",
    "    i0[\"summed_instances\"] = sum([i0[col.replace(\"rate_\", \"num_instances_\")] for col in valid_model_keys])\n",
    "    \n",
    "    #Aggregate some stats for GPU instances\n",
    "    valid_gpu_keys = [col for col in i0.columns if col.startswith(\"gpu_tensor_util\") and col.replace(\"tensor\", \"dram\") in i0.columns]\n",
    "    i0[\"summed_gpu_tensor_util\"] = sum([i0[col] for col in valid_gpu_keys])\n",
    "    i0[\"summed_gpu_dram_util\"] = sum([i0[col.replace(\"tensor\", \"dram\")] for col in valid_model_keys])\n",
    "    return i0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a8b3d-7d38-4b98-8a3a-2d283f4990e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i0 = convert_results_to_df(results)\n",
    "i0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243ca52-500b-408c-91c3-ab5393c1b1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(i0.index.values, i0.rate.values)\n",
    "scale_value = max(i0.rate.values)/max(i0.summed_gpu_tensor_util)\n",
    "plt.plot(i0.index.values, i0.summed_gpu_tensor_util.values*scale_value, color=\"tab:red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e02198-b119-4df0-a73d-66e8770f5860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"triton_metrics_test.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(i0, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b912b07-fbd3-431b-8d1c-330b63f73aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Concurrency question: if models tend to gravitate to their own instances, summed instances ~ num_instances\n",
    "#If concurrency is as high as possible, summed instances ~ avg_num_models * num_instances\n",
    "ii = i0[i0.num_instances > 0].summed_instances/i0[i0.num_instances > 0].num_instances\n",
    "print(np.mean(ii), np.max(ii), np.min(ii))\n",
    "print(np.sqrt(np.var(ii)))\n",
    "\n",
    "#Consistency check: summed rate should always add to net rate!\n",
    "kk = i0[i0.num_instances > 0].summed_rate/i0[i0.num_instances > 0].rate\n",
    "print(np.mean(kk), np.max(kk), np.min(kk))\n",
    "print(np.sqrt(np.var(kk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6c219-49ca-49d6-ab2a-e9c00707dcd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(\"num_instances\", \"rate\", data=i0[i0.num_instances > 0], color=\"tab:red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58424494-1247-4eee-9b46-7867e6ef21b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(\"num_instances_pn_demo\", \"rate_pn_demo\", data=i0[i0.num_instances > 0], color=\"tab:blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ddc95-759c-4451-ae45-ab313e4b42c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(\"num_instances_svj_tch_gnn\", \"rate_svj_tch_gnn\", data=i0[i0.num_instances > 0], color=\"tab:green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e125dc-31af-4d94-828b-a588d07877de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea for triton",
   "language": "python",
   "name": "coffea-triton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
